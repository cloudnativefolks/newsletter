---
layout: post
title: "KubeCon2024 Edition: CloudNativeFolks Community"
categories: Newsletter 
--- 

### KubeCon24 PARIS, FRANCE

<iframe width="720" height="720" src="https://www.youtube.com/embed/1u5LtsJqyrA?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Project Carvel: Composable Tools for Application Management" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 1.Project Carvel: Composable Tools for Application Management

Project Carvel provides a suite of reliable, single-purpose, and composable tools designed for application management. This comprehensive overview highlights the functionalities and use cases of various tools within the Project Carvel ecosystem, demonstrating their practical applications through examples.

- **Composability and Reliability**: The core philosophy behind Project Carvel is to offer a set of tools that are both composable and reliable. Each tool is designed to perform a specific task, ensuring simplicity and effectiveness in managing applications.

- **Tools Overview**:
    - **ytt (YAML Templating Tool)**: ytt is a powerful tool for templating YAML configurations. It understands YAML as a data structure, enabling users to manipulate YAML in unique ways. ytt is versatile, supporting not just Kubernetes resources but also any YAML-based configurations like GitHub actions.
    - **kapp**: kapp is used for deploying and managing resources on Kubernetes clusters. It treats resources as a group and understands the dependencies between them, ensuring that resources are applied in a sensible order and reflecting the desired state accurately.
    - **kbld (Kubernetes Build)**: kbld focuses on managing image references in configuration files. It resolves image tags to immutable digests, ensuring that the exact images used during development are the ones deployed to production.
    - **imgpkg**: imgpkg is designed for bundling, distributing, and versioning configurations using OCI images. It allows for the creation of versioned bundles that can be relocated across registries, supporting air-gapped environments and ensuring integrity through immutability.
    - **Carvel Packaging APIs**: These APIs facilitate the distribution and installation of software across Kubernetes clusters. They provide a higher-level abstraction over the lower-level tools, enabling easier management and deployment of applications.

- **Practical Demonstration**: The presenters showcased practical uses of each tool, starting with ytt to template and manipulate YAML configurations, then moving to kapp for deployment management. They demonstrated how kbld secures image references and how imgpkg bundles configurations for distribution. Finally, they discussed the Carvel Packaging APIs, which streamline the deployment process across different environments.

<iframe width="720" height="720" src="https://www.youtube.com/embed/3OSQdiKTNU8?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="When They Go High, We Go Low – Hooking Libc Calls to Debug Kubernetes Apps - Tal Zwick, MetalBear" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 2.When They Go High, We Go Low – Hooking Libc Calls to Debug Kubernetes Apps - Tal Zwick, MetalBear

This talk, presented by Tal Zwick from MetalBear, delves into innovative low-level techniques used to enhance the development experience of cloud-native applications through a tool named Mird, developed in Rust. The primary focus is on addressing the challenges cloud-native developers face, emphasizing the significance of running and debugging code in environments closely resembling production settings.

### **Key Points:**

- **Objective**: The main challenge tackled is improving the cloud-native developer experience, particularly the ease of running and debugging code. Mird offers a unique solution by enabling developers to execute applications locally while still interacting with cluster resources, bridging the gap between local development and cloud-native environments.

- **Mird Overview**: Mird facilitates the local execution of cloud-native applications with seamless cluster integration. This approach allows developers to maintain productive development practices without compromising the complexity inherent in cloud-native applications. It essentially merges the simplicity of local development with the robustness of cloud-native operations.

- **Technical Insights**: A considerable portion of the talk is dedicated to the technical underpinnings of Mird. Key techniques include:
    - Hooking libc functions using the dynamic instrumentation toolkit, Frida, to intercept and manipulate system calls, directing them towards cluster resources when necessary.
    - Employing dynamic libraries for process-level virtualization, creating a transparent connection between the local development environment and cluster resources.
    - Utilizing environment variables and configuration flags to determine the execution context (local vs. cluster) for specific operations.

- **Development Workflow**: Mird enhances the development workflow by allowing for real-time debugging and interaction with live data and services within the cluster. This setup supports a more intuitive and efficient debugging process, facilitating immediate feedback and iteration.

<iframe width="720" height="720" src="https://www.youtube.com/embed/9EARwoRStBY?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Keynote: Cloud Native in its Next Decade - Davanum Srinivas &amp; Lin Sun" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 3.Keynote: Cloud Native in its Next Decade - Davanum Srinivas &amp; Lin Sun

The keynote speech, presented by Davanum Srinivas and Lin Sun, delves into the progression and future direction of cloud-native technologies over the next decade. Their presentation covers a comprehensive overview of the Cloud Native Computing Foundation's (CNCF) achievements, project incubations, and the evolving landscape of cloud-native technologies. Here are the key insights from their discussion:

- **Community and Governance:** The CNCF's Technical Oversight Committee (TOC) plays a crucial role in guiding the foundation's technical direction. With new members added, the committee ensures diverse representation and governance across cloud-native projects.

- **Project Evolution:** Highlighting the growth from incubation to graduation, the presentation underscores the importance of a clear pathway for project development within the CNCF ecosystem. This process promotes stability, maturity, and innovation among cloud-native projects.

- **Historical Reflections:** A look back at the architecture and key milestones since 2015 reveals the rapid evolution of cloud-native technologies. From the inception of Kubernetes and Prometheus to the introduction of newer projects like K3s and WASM, the landscape has continuously expanded and diversified.

- **Future Directions:** The keynote speculates on the next decade of cloud-native technology, emphasizing areas like AI, sustainability, security, edge computing, and service mesh. It suggests that these technologies will remain at the forefront of cloud-native innovation.

- **Big Disruptions:** Predictions about the next big disruptions in cloud-native include the integration of Cloud Native and AI, simplicity in technology, and the need for consolidation amidst the proliferation of projects. Additionally, the emphasis on developer-friendly APIs and sustainable, efficient infrastructure is expected to shape the future.

- **Community Engagement:** The speech encourages community engagement and participation in shaping the future of cloud-native technologies. It concludes with an invitation to join a panel discussion with TOC members to further explore these themes.

Insights based on numbers:
- **184 Projects:** The CNCF has grown to encompass 184 projects, indicating the vast scale and diversity of the cloud-native ecosystem.
- **Decade of Growth:** Reflecting on the past decade, the presentation provides a roadmap for the next, highlighting the key areas of technological advancement and community focus.

<iframe width="720" height="720" src="https://www.youtube.com/embed/D0jE4BSzX3Y?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Prometheus Update from the Maintainers - Bryan Boreham, Grafana Labs &amp; Simon Pasquier, Red Hat" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 4.Prometheus Update from the Maintainers - Bryan Boreham, Grafana Labs &amp; Simon Pasquier, Red Hat

The presentation by Bryan Boreham and Simon Pasquier provides an in-depth update on Prometheus, covering its evolution, recent enhancements, and future directions. As an integral part of the cloud-native monitoring landscape, Prometheus has continued to grow and adapt to meet the needs of its users. Below are the key insights and updates shared by the maintainers:

- **Community Growth:** Prometheus has seen substantial growth in its community and contributions, reflecting its widespread adoption and the active involvement of its user base.

- **Recent Developments:**
  - **Releases and Support:** Over the past year, Prometheus made significant progress with eight releases, including the introduction of Long-Term Support (LTS) versions for users seeking stability.
  - **Client and Exporter Updates:** The Java client reached 1.0, marking a major milestone with a comprehensive rearchitecture. The Go client and various exporters saw numerous releases, indicating ongoing enhancements.

- **Feature Enhancements:**
  - **Native Histograms:** A major focus has been on improving histogram resolution and efficiency. While still experimental, native histograms promise more detailed and accurate data representation.
  - **OpenTelemetry Integration:** An experimental ingestion endpoint for OpenTelemetry metrics has been introduced, signaling closer integration with the broader observability ecosystem.

- **Performance Improvements:** Efforts to reduce Prometheus's memory footprint and improve efficiency have been successful, with further optimizations ongoing, such as duplicate label reduction.

- **Future Roadmap:**
  - **Version 3.0:** Plans for Prometheus 3.0 include solidifying experimental features into the core offering and simplifying the upgrade process for users. The transition from version 2 to 3 aims to be seamless, building on the project's commitment to not breaking user workflows.
  - **Enhanced OpenTelemetry Support:** Enhanced support for OpenTelemetry metrics, including compatibility with UTF-8 metric and label names, aims to simplify integration and use.
  - **UI Improvements:** A refreshed user interface is in development, focusing on a cleaner and more efficient design without drastically altering the user experience.

- **Community and Contribution:** The maintainers emphasized the importance of community contributions and engagement. They highlighted opportunities for new contributors to get involved, including a dedicated session for contributors at the conference.

Insights based on numbers:
- **12 Years of Development:** Reflecting on its long history, Prometheus has matured significantly, joining the CNCF in 2016 and reaching graduation two years later.
- **Growth in Installations:** The presentation showcased a steady increase in Prometheus installations, indicating its widespread acceptance and use across various industries.

<iframe width="720" height="720" src="https://www.youtube.com/embed/JFLNFJT59DY?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="GitOps Continuous Delivery at Scale with Flux - Stefan Prodan" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>


### 5.GitOps Continuous Delivery at Scale with Flux - Stefan Prodan

Stefan Prodan's talk on "GitOps Continuous Delivery at Scale with Flux" provides an insightful overview of the Flux project, its roadmap, and strategies for scaling GitOps practices. Flux, as a foundation layer for continuous delivery platforms, is designed to extend Kubernetes in numerous ways, offering flexibility and scalability in managing deployments. Key insights from the talk include:

- **Flux Definition and Capabilities:**
  - Flux is not just a platform but a Kubernetes extension that includes 13 Custom Resource Definitions (CRDs) and six controllers, emphasizing its extensibility and adaptability to various use cases and environments.

- **Scaling Strategies:**
  - Prodan outlines a journey for scaling Flux, starting with source optimization and control fine-tuning, moving to vertical scaling, and ultimately considering horizontal scaling through sharding for larger deployments.

- **Roadmap Highlights:**
  - The roadmap for Flux includes reaching general availability for 80% of its APIs, integrating notary and CD events for improved security and interoperability, and enhancements to Helm OCI support.

- **Sustainability and Community Involvement:**
  - The sustainability of Flux is seen as a collaborative effort within the community. Prodan encourages contributions and a new model for maintainership focused on specific features to foster a broader base of maintainers and specialists.

Insights based on numbers:
- **Benchmarks and Performance:**
  - Prodan shares benchmarking results showing Flux's capability to handle 1K Helm releases in about 8 minutes and 1K customizations in around 4 minutes, highlighting its efficiency in managing large-scale deployments.
<iframe width="720" height="720" src="https://www.youtube.com/embed/KaXIq8Qv77A?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Shift-Left: Past, Present, and Future of Validation in CI... Alexander Zielenski &amp; Stefan Schimanski" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 6.Shift-Left: Past, Present, and Future of Validation in CI

The presentation by Alexander Zielenski and Stefan Schimanski on "Shift-Left: Past, Present, and Future of Validation in CI" explores the evolution of validation mechanisms in Kubernetes and the continuous effort to improve developer experience by integrating validation more deeply into the development process. Here are the key insights from their discussion:

- **Evolution of Validation:** 
  - The talk begins with a historical overview, highlighting how validation has been a core aspect of Kubernetes since its inception. Initially, validation was primarily server-side, relying on the Kubernetes API server to catch errors.

- **Shift-Left Concept:**
  - The "shift-left" concept refers to moving validation closer to the development phase, ideally providing immediate feedback to developers. This approach aims to identify and rectify errors early in the CI/CD pipeline, enhancing efficiency and reducing the iteration time.

- **Challenges with Current Validation Methods:**
  - The presenters discuss the limitations of existing validation methods, including client-side validation with `kubectl` and server-side dry run validations. These methods either lack comprehensive error reporting or require access to a Kubernetes cluster, which can be impractical in CI environments.

- **Introduction of CEL (Common Expression Language):**
  - To address these limitations, Kubernetes has introduced support for the Common Expression Language (CEL) in Custom Resource Definitions (CRDs). CEL allows for more complex and expressive validations, including field immutability and conditional logic.

- **Automated Validation Ratcheting:**
  - Another significant development is automated validation ratcheting, which allows CRDs to be updated with stricter validation rules without breaking existing resources. This feature facilitates the gradual strengthening of validation rules over time.

- **Future Directions:**
  - The future of validation in Kubernetes focuses on further enhancing the expressiveness and applicability of CRD validations. Efforts are underway to encode native Kubernetes type validations using CEL, aiming to provide consistent and comprehensive validation across all resource types.

- **Tooling for Enhanced Validation:**
  - The presentation introduces `kubectl validate`, a new tool designed to leverage these advancements in validation technology. `kubectl validate` aims to provide instant feedback on validation errors, using the same validation logic as the Kubernetes API server but executed client-side.

Insights based on numbers:
- **Performance and Efficiency:** The introduction of CEL and automated validation ratcheting represents a significant leap in the performance and efficiency of Kubernetes validation mechanisms, enabling more complex validations without compromising on feedback speed or accuracy.

<iframe width="720" height="720" src="https://www.youtube.com/embed/TOX8UCnKHWo?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Composable Systems in Kubernetes - Michele Gazzetti, IBM Research Europe - Ireland" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 7.Composable Systems in Kubernetes - Michele Gazzetti, IBM Research Europe - Ireland

Michele Gazzetti's presentation on "Composable Systems in Kubernetes" explores the integration and management of composable disaggregated infrastructure (CDI) within Kubernetes environments. Gazzetti discusses the challenges and developments in creating more flexible and efficient systems by dynamically allocating and managing resources such as compute, memory, accelerators, and storage. Here are the key insights from his discussion:

- **CDI Overview:**
  - CDI refers to a set of disaggregated physical resources that can be dynamically allocated and managed via a fabric. This approach allows for more efficient use of resources by adjusting allocations based on current needs.

- **Challenges of CDI Integration:**
  - Integrating CDI into Kubernetes introduces complexity in managing resources at a granular level. The primary challenge is to abstract the complexity of CDI management from the end-users and provide a seamless interface within Kubernetes for resource allocation.

- **Sunfish Framework:**
  - Sunfish, an open framework under the OpenFabrics Alliance, aims to address these challenges by abstracting vendor-specific APIs and providing a unified view of available resources. It manages the dynamic selection and reconfiguration of resources based on user-defined policies.

- **Composable Resource Operator:**
  - The Composable Resource Operator is a proof-of-concept developed to integrate CDI within Kubernetes. It allows users to request composable resources, like GPUs, through Kubernetes Custom Resource Definitions (CRDs). The operator manages the attachment and detachment of these resources to nodes.

- **Demo and Implementation:**
  - A demonstration showcases the dynamic allocation of GPUs to a Kubernetes node based on a pod's requirements. The process involves creating a "composability request" through a CRD, which triggers the allocation of resources from the pool to the specified node.

- **Management Complexity:**
  - Successfully integrating CDI requires managing the lower levels of the stack and ensuring compatibility with various CDI solutions from different vendors. This includes handling the attachment and detachment processes and ensuring that resources are recognized and utilized effectively by Kubernetes.

- **Future Directions:**
  - Future work will focus on enhancing compatibility with the Sunfish framework, exploring various CDI solutions, and investigating the sustainability impact of CDI in data centers. Other areas of interest include addressing scheduling topology constraints and finding ways to merge the views of Kubernetes scheduling with physical infrastructure constraints.

Insights based on numbers:
- **Resource Efficiency and Dynamic Allocation:**
  - CDI's dynamic allocation mechanism offers a more efficient utilization of data center resources by reducing over-provisioning and enabling the precise allocation of high-value resources like GPUs.
 
<iframe width="720" height="720" src="https://www.youtube.com/embed/ZcgFgb8dLOw?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Distributed AI: Using NATS.Io to Seamlessly Connect AI Applications... Tomasz Pietrek &amp; Jeremy Saenz" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 8.Composable Systems in Kubernetes - Michele Gazzetti, IBM Research Europe - Ireland

Tomasz Pietrek and Jeremy Saenz's presentation introduces the utilization of NATS.io for connecting AI applications, emphasizing its application in distributed AI systems. NATS.io, a high-performance messaging system, is showcased as a pivotal technology for AI at the edge, offering a simplified, efficient, and scalable solution for AI workloads. The presentation covers key features, recent updates to NATS, and a practical demonstration of AI model inference using NATS for messaging. Here are the detailed insights:

- **NATS Overview:**
  - **Core Functions:** NATS provides basic messaging patterns like publish/subscribe and request/reply, foundational for distributed system communication. Its capabilities extend to persistence through JetStream, facilitating at-least-once delivery semantics.
  - **Distributed System Design:** The emphasis on distributed system design highlights NATS's strengths in connecting various components seamlessly, including cloud, data center, and edge devices. NATS simplifies the architecture by offering unified messaging for data exchange, service discovery, and configuration management across disparate environments.

- **Enhancements and Use Cases:**
  - **Tracing and Monitoring:** The introduction of tracing in the upcoming NATS version promises enhanced visibility into message flows across multi-layered architectures, vital for debugging and performance optimization in AI workloads.
  - **Dynamic Resource Allocation:** Through its key-value stores and JetStream, NATS supports dynamic resource allocation, essential for managing AI model deployments and data streams in real-time.

- **AI at the Edge Demonstration:**
  - The live demonstration focused on AI model inference for object detection, showcasing NATS's role in facilitating efficient data transfer between edge devices and AI models. The use of NATS reduced latency and improved the responsiveness of AI applications, demonstrating its effectiveness in edge computing scenarios.

- **Operational Simplicity and Scalability:**
  - **Lightweight and Flexible:** NATS's operational simplicity, with its small binary size and comprehensive client support, underscores its adaptability to various deployment scenarios, from cloud-based services to edge devices.
  - **Leaf Nodes and Global Networks:** The discussion on leaf nodes and the global network architecture of NATS highlighted its scalability and flexibility, enabling seamless integration of local and remote systems for distributed AI applications.

- **Community and Future Directions:**
  - **Community Contributions:** The presentation encouraged community engagement, with an open invitation for contributions to NATS and related projects. The focus on community-driven development is poised to accelerate the evolution of NATS and its ecosystem.
  - **Sustainability and Innovation:** Looking forward, the speakers envisioned exploring NATS's potential to enhance data center sustainability through intelligent resource management and to address complex scheduling and connectivity challenges in distributed AI environments.

Insights based on numbers:
- **Performance Metrics:** Benchmarking demonstrations showcased NATS's high throughput capabilities, essential for the data-intensive requirements of AI workloads.

<iframe width="720" height="720" src="https://www.youtube.com/embed/euYhIn4leW0?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Observable Feature Rollouts with OpenTelemetry and OpenFeature - Daniel Dyla &amp; Michael Beemer" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 9.Observable Feature Rollouts with OpenTelemetry and OpenFeature - Daniel Dyla &amp; Michael Beemer

Daniel Dyla and Michael Beemer’s presentation focuses on leveraging OpenTelemetry and OpenFeature for observability in feature rollouts, offering insights into managing and monitoring feature flags in a distributed microservices architecture. Their talk outlines the integration of these tools to enhance deployment strategies and mitigate risks associated with new features. Here are the detailed insights:

- **OpenFeature Introduction:**
  - **Purpose:** OpenFeature is an open specification for vendor-agnostic, community-driven feature flags. It aims to standardize feature flag management across different environments and systems, supporting both commercial and open-source tools.
  - **Feature Flags:** Feature flags serve as pivot points in code that can be dynamically adjusted without deploying new code versions, facilitating decoupled release strategies and risk management.

- **Feature Flag Benefits and Challenges:**
  - **Benefits:** Feature flags enable decoupled deployments, risk reduction by controlling the impact radius, and experimental A/B testing with different feature variations.
  - **Challenges:** The primary challenge highlighted is the complexity added to the system, especially in a microservices architecture where multiple feature flags can alter code paths dynamically, necessitating enhanced monitoring.

- **OpenTelemetry for Monitoring:**
  - **Functionality:** OpenTelemetry provides a suite of APIs and SDKs for collecting telemetry data (events, traces, metrics) in a vendor-neutral manner, allowing for comprehensive observability across services.
  - **Data Collection:** The talk underscores the importance of selecting the appropriate telemetry signals based on the volume of data, structured vs. unstructured data, and the specific analysis needs.

- **Observable Feature Rollouts:**
  - The speakers present a hypothetical scenario involving a sneaker shop to illustrate the process of safely rolling out a new database read replica feature behind a feature flag.
  - **Process Steps:** The process includes adding the feature flag, monitoring its impact via OpenTelemetry, controlling the rollout to a subset of users, and analyzing the telemetry data to identify and resolve issues before a full rollout.

- **Practical Demonstration:**
  - **Integration:** A demonstration shows how OpenFeature and OpenTelemetry work together to monitor feature flag evaluations and their impact on system behavior, utilizing traces to analyze and troubleshoot issues.
  - **Rollback Mechanism:** The ability to quickly rollback a feature for specific users based on observed telemetry data highlights the dynamic control feature flags offer in production environments.

- **Conclusion and Best Practices:**
  - **Comprehensive Monitoring:** The combination of OpenFeature for feature flag management and OpenTelemetry for observability enables a robust strategy for feature rollouts, emphasizing the need for continuous monitoring and analysis.
  - **Future Observability:** The presentation concludes with a Q&A session, addressing inquiries about automated rollbacks, handling feature flag-related metrics to avoid cardinality issues, and the support for these practices in existing tools.

Insights based on numbers:
- **Performance Improvements:** The hypothetical scenario showcased how implementing a read replica could significantly reduce database response times, demonstrating the tangible benefits of observable feature rollouts.

<iframe width="720" height="720" src="https://www.youtube.com/embed/hENwFyrtm1g?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Open Policy Agent (OPA) Intro &amp; Deep Dive - Anders Eknert, Styra &amp; Xander Grzywinski, Microsoft" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 10. Open Policy Agent (OPA) Intro &amp; Deep Dive - Anders Eknert, Styra &amp; Xander Grzywinski, Microsoft

The presentation by Anders Eknert and Xander Grzywinski offers a comprehensive introduction to the Open Policy Agent (OPA) and a deep dive into its functionalities, benefits, and the ecosystem. The talk addresses the fundamentals of OPA, project updates, and its integration within the Kubernetes environment through the OPA Gatekeeper project. Here are the detailed insights:

- **OPA Fundamentals:**
  - **Definition and Purpose:** OPA is an open-source, general-purpose policy engine designed to unify policy enforcement across the cloud-native stack. It decouples policy from application logic, allowing policies to be managed and enforced independently.
  - **Policy as Code:** OPA emphasizes treating policy as code, supporting practices like code review, testing, and deployment automation for policy management. This approach facilitates collaboration and centralizes policy governance.
  - **Rego Policy Language:** Policies are expressed in Rego, a declarative language developed specifically for OPA, enabling fine-grained, context-aware policy definition.

- **Project Updates and Roadmap:**
  - **OPA 1.0 Release:** The upcoming release aims to address language ambiguities and introduce syntactical improvements for better readability and maintainability of Rego policies.
  - **Future Features:** The roadmap includes runtime schema validation, enhanced configuration options, improved testing output for OPA policies, and new language features to facilitate policy writing and debugging.

- **OPA Gatekeeper for Kubernetes:**
  - **Introduction:** OPA Gatekeeper extends OPA's capabilities to Kubernetes, offering a Kubernetes-native policy management solution. It leverages OPA for validating and mutating admission control, enforcing policies at the Kubernetes API level.
  - **Core Features:** Gatekeeper introduces constraint templates for policy definition, supports data mutation through custom resource definitions (CRDs), and provides auditing capabilities to ensure compliance with policies across Kubernetes resources.

- **Integration and Use Cases:**
  - **Admission Control:** Demonstrates how Gatekeeper can enforce policies on Kubernetes resources during the admission process, preventing non-compliant resources from being deployed.
  - **Mutation:** Gatekeeper can mutate Kubernetes objects based on specified policies, enabling automatic adjustments to resources, such as adding labels or annotations.
  - **Auditing:** Offers insights into existing resources' compliance, highlighting violations and facilitating governance across Kubernetes clusters.

- **Community and Ecosystem:**
  - **Vibrant Community:** OPA boasts a strong community of users and contributors, with extensive integrations across the cloud-native ecosystem, emphasizing the project's maturity and wide adoption.
  - **Tooling and Editor Support:** The ecosystem includes tools for writing, testing, and deploying Rego policies, with editor integrations available for a seamless development experience.

Insights based on numbers:
- **Adoption Metrics:** OPA's widespread adoption is underscored by its GitHub stars, Slack users, and download numbers, reflecting its importance in the cloud-native landscape.

<iframe width="720" height="720" src="https://www.youtube.com/embed/jaC9eVEcwzk?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="The Chain of Trust: Towards SLSA L3 with Tekton Trusted Artifacts - Jerop Kipruto &amp; Andrea Frittoli" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 11. The Chain of Trust: Towards SLSA L3 with Tekton Trusted Artifacts - Jerop Kipruto &amp; Andrea Frittoli

Andrea Frittoli's presentation delves into enhancing the chain of trust in the software development lifecycle (SDLC) with Tekton, focusing on achieving Supply-chain Levels for Software Artifacts (SLSA) Level 3 compliance. Tekton, a powerful cloud-native CI/CD framework, plays a crucial role in establishing a trusted environment for building, testing, and deploying software. Key insights from the talk include:

- **Chain of Trust in SDLC:** The chain of trust concept ensures reliability at each step of the SDLC, from code writing to deployment. A single breach in this chain can compromise the entire process, underscoring the need for stringent security measures.

- **SLSA Framework:** SLSA provides a security framework defining levels of trust for software artifacts. Frittoli highlighted the focus on SLSA Level 3 for builds, which requires a hardened, secure build platform and provenance for artifacts.

- **Tekton Overview:** Tekton extends Kubernetes to model CI/CD pipelines using custom resources, offering a scalable and secure way to automate software delivery. It supports security features like artifact signing and verifying to enhance trust.

- **Tekton Trusted Artifacts:** The concept of "Tekton Artifacts" was introduced to further secure the SDLC by ensuring the integrity of artifacts passed between steps in a Tekton pipeline. This initiative aims to meet SLSA Level 3 requirements by attesting to each step's inputs and outputs, providing a verifiable chain of custody for artifacts.

- **Demonstration:** Frittoli presented a demo highlighting potential security vulnerabilities when using shared workspaces without proper artifact attestation. The demonstration showed how Tekton Artifacts could mitigate these risks by ensuring each artifact's provenance is verified before it is consumed by subsequent steps.

- **Future Directions:** The presentation outlined upcoming features for Tekton, including step attestation, artifact definition in the Tekton API, and the integration of standards like SPIFFE, in-toto, and CD Events. These features aim to provide stronger guarantees about the security and provenance of software artifacts.

Insights based on numbers:
- **Community and Adoption:** Tekton's robust community and wide adoption among major companies underscore its importance as a tool for building secure, cloud-native CI/CD pipelines.

<iframe width="720" height="720" src="https://www.youtube.com/embed/lswkWQU4_W0?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Enabling Coordinated Checkpointing for Distributed HPC Applicati... Radostin Stoyanov &amp; Adrian Reber" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 12. Enabling Coordinated Checkpointing for Distributed HPC Applications - Radostin Stoyanov & Adrian Reber 

The presentation by Radostin Stoyanov and Adrian Reber introduces advancements in enabling coordinated checkpointing for distributed High-Performance Computing (HPC) applications, utilizing the Checkpoint/Restore in Userspace (CRIU) tool and Kubernetes. Their work focuses on overcoming the limitations of traditional checkpointing methods by facilitating a synchronized, distributed checkpointing mechanism that is critical for fault tolerance and efficient computation in HPC environments. Here are the detailed insights:

- **Checkpoint/Restore in Userspace (CRIU):** CRIU is a tool that enables the checkpointing and restoring of Linux processes in userspace. Originally designed for container migration, CRIU has been pivotal in enabling stateful snapshotting and restoration of applications, making it foundational for the discussed advancements.

- **Challenges in Distributed HPC Applications:**
  - Traditional checkpointing methods, while effective for single-node applications, fall short in distributed settings where coordinated state snapshotting across multiple nodes is required.
  - The need for a coordinated checkpointing mechanism arises from the complexity of HPC applications, which often span across numerous computational nodes, necessitating a synchronized state capture to ensure consistency and fault tolerance.

- **Coordinated Checkpointing Mechanism:**
  - The researchers have developed a coordination tool that interfaces with CRIU to enable synchronized checkpointing across distributed systems. This tool utilizes CRIU's action script hooks to pause and resume processes, ensuring a consistent global state is captured.
  - The coordination tool registers CRIU instances and manages dependencies between them, allowing for a coordinated checkpoint and restore process that aligns with the needs of distributed HPC applications.

- **Integration into Kubernetes:**
  - The present work aims to integrate these checkpointing capabilities into Kubernetes, leveraging the container orchestration platform to manage and automate checkpointing tasks.
  - Kubernetes' extensibility and wide adoption make it an ideal platform for deploying HPC applications that benefit from coordinated checkpointing, providing resilience and flexibility in managing computational workloads.

- **Demonstration and Use Cases:**
  - A demonstration showcased the migration of a containerized application across nodes, illustrating the potential of CRIU and the coordination tool in facilitating stateful container migrations.
  - Use cases for this technology extend beyond simple migrations, including fault tolerance in long-running computations, dynamic resource allocation, and load balancing in cloud environments.

- **Future Work and Challenges:**
  - The presentation outlines future directions, including the refinement of the coordination mechanism, deeper integration with Kubernetes' ecosystem, and enhancements to support a broader range of distributed computing patterns.
  - Addressing the challenges of capturing and restoring state in distributed systems, especially regarding network connections and external dependencies, remains an area of active research.

Insights based on numbers:
- **Advancement in CRIU and Kubernetes:** The work represents significant progress in the integration of CRIU within Kubernetes, offering a path toward robust, coordinated checkpointing in distributed systems.

<iframe width="720" height="720" src="https://www.youtube.com/embed/mDPLioNIIjo?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="CRI-O Odyssey: Exploring New Frontiers in Container Runtimes - Julien Ropé &amp; Sohan Kunkerkar" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 13.CRI-O Odyssey: Exploring New Frontiers in Container Runtimes - Julien Ropé &amp; Sohan Kunkerkar

The presentation by Julien Ropé and Sohan Kunkerkar focuses on the latest advancements and future directions in CRI-O, a lightweight container runtime for Kubernetes. The speakers delve into several key areas, including the project's community growth, technical enhancements, and innovative features aimed at addressing the evolving needs of containerized applications. Here are the detailed insights:

- **Community Growth and CNCF Graduation:**
  - The project has witnessed steady growth in contributions and interest, as evidenced by the increasing number of GitHub stars. This growth underscores the community's robust engagement and the project's maturation.

- **Upcoming Features in Release 1.30:**
  - Enhancements include easier deployment of Seccomp profiles, support for s390 architecture, and the introduction of a split image filesystem. These improvements aim to enhance security, compatibility, and resource efficiency.

- **Confidential Containers:**
  - A significant focus is on integrating confidential containers into CRI-O, which ensures workload confidentiality by encrypting memory in use. This approach leverages virtual machines running on hypervisors with hardware support for memory encryption, enhancing the security of sensitive workloads.

- **Wasm Integration:**
  - The integration of WebAssembly (Wasm) with CRI-O and Kubernetes opens new possibilities for deploying lightweight, secure, and portable applications across a variety of architectures and environments, including edge computing scenarios.

- **Rootless Containers and Enhanced Security:**
  - CRI-O is advancing in supporting rootless containers, which run without root privileges, enhancing security and minimizing the risk of privilege escalation attacks. This effort includes developments in proc mount options and username space pods, further securing containerized applications.

- **Split Image Filesystem and Space Efficiency:**
  - The introduction of a split image filesystem in CRI-O addresses disk space concerns by separating read-only image layers from writable layers. This feature not only optimizes space usage but also improves the isolation of containers.

- **Future Directions:**
  - Future work includes automatic reloading of mirror config for registry, implementation of a Rust-based NRI framework, direct loading of Wasm plugins into CRI-O, and exploring FreeBSD support. These initiatives aim to expand CRI-O's capabilities and address the needs of complex containerized environments.

Insights based on numbers:
- **Technical Enhancements:** The continuous introduction of new features and support for additional architectures and security mechanisms illustrates CRI-O's commitment to meeting the diverse needs of Kubernetes environments.

<iframe width="720" height="720" src="https://www.youtube.com/embed/q8AVAwdCqYk?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Create Cloud Native Agents and Extensions for LLMs - Xiaowei Hu, Second State" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 14. Create Cloud Native Agents and Extensions for LLMs - Xiaowei Hu, Second State

Xiaowei Hu's session focuses on leveraging WebAssembly (Wasm) to create efficient, cloud-native agents and extensions for Large Language Models (LLMs), with an emphasis on overcoming the challenges posed by traditional deployment methods. Here are the detailed insights:

- **Challenges with Python and Native Applications:**
  - **Python's Heavyweight Nature:** Python, commonly used for AI and LLM tasks, presents issues due to its large, complex dependencies. For instance, the Docker image for Python can be as large as the LLM itself, leading to inefficiency.
  - **Portability Issues with Native Applications:** While native applications in languages like Rust or C offer size advantages over Python, they lack portability. Recompiling applications for different environments (e.g., macOS vs. NVIDIA GPU) is necessary, a limitation in cloud-native settings where Kubernetes predominantly operates on binary artifacts.

- **Leveraging WebAssembly for Cloud-Native Development:**
  - **WebAssembly as a Solution:** The session introduces WebAssembly (Wasm) and the WebAssembly System Interface (WASI) as solutions to the portability and efficiency challenges. Wasm provides an abstraction layer between applications and underlying hardware, allowing for "write once, run anywhere" capabilities across different computing environments.
  - **WASI for System Functions:** WASI extends WebAssembly's capabilities to non-browser environments, enabling access to system functions like networking and file systems. This is crucial for AI and LLM workloads, which often require intensive computational resources.

- **Integration with Large Language Models:**
  - **Creating Lightweight Inference Applications:** By abstracting GPU drivers and inference frameworks through Wasm, developers can create applications that are not only portable but also significantly smaller in size compared to traditional Python-based solutions. This approach reduces the deployment size from gigabytes to megabytes, enhancing deployment efficiency.
  - **Supporting a Range of AI and LLM Workloads:** The session demonstrates how WebAssembly and WASI can support a diverse range of AI models and frameworks, including PyTorch, TensorFlow, and large language models like GPT-3. This versatility is showcased through the deployment of LLM applications on various hardware platforms, highlighting the flexibility and efficiency of the Wasm approach.

- **Cloud-Native Deployments and Kubernetes Integration:**
  - **Kubernetes Integration:** A key part of the discussion is the integration of Wasm applications with Kubernetes, allowing for the deployment of Wasm-based AI and LLM workloads in a cloud-native environment. This integration enables the utilization of Kubernetes' orchestration capabilities for managing Wasm applications, ensuring efficient resource utilization and scalability across cloud environments.

- **Demonstrations and Use Cases:**
  - **Demo Applications:** The presentation includes demonstrations of deploying LLM applications using Wasm, showcasing the process of compiling applications to Wasm bytecode and deploying them across different devices. These demos underline the practical benefits of using Wasm for cloud-native AI and LLM deployments, including portability, efficiency, and integration with Kubernetes.

- **Future Directions:**
  - **Expanding Wasm's Ecosystem:** The session hints at future developments in expanding the ecosystem of tools and frameworks supporting Wasm for AI and LLM applications. This includes enhancing the support for different AI models, optimizing performance for various hardware platforms, and further integrating with cloud-native technologies.

Insights based on numbers:
- **Reduction in Application Size:** The transformation from gigabyte-sized Docker images to megabyte-sized Wasm applications represents a significant leap in deployment efficiency, crucial for cloud-native environments.

<iframe width="720" height="720" src="https://www.youtube.com/embed/rJ6vebxS0aI?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Tackling Configuration Management at Scale with Flux, CUE and OCI at... Alec Hothan &amp; Stefan Prodan" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 15.Tackling Configuration Management at Scale with Flux, CUE and OCI at Cisco

The presentation by Alec Hothan and Stefan Prodan addresses complex configuration management for Kubernetes at scale, leveraging Flux, CUE, and OCI artifacts. Their insights cover the deployment challenges at Cisco with hundreds of clusters, integrating diverse workloads, and strategies for effective GitOps practices. Below are the highlighted points:

- **Challenges and Scale:**
  - **Deployment Scope:** Cisco faced the challenge of deploying a wide variety of workloads across hundreds of clusters worldwide, including on-premises and cloud environments. The scale required a robust configuration management solution to handle variations across different data centers and cluster types.

- **Flux for GitOps:**
  - **Adopting Flux:** Flux was chosen for its GitOps capabilities, allowing teams to manage Kubernetes resources through Git repositories. This approach facilitated collaboration among over 200 team members, including developers, testers, and operations teams.
  - **Multi-level Git Repositories:** To manage configurations for different environments, a two-level Git repository strategy was employed. This setup separated application source code and deployment configurations, enabling more controlled updates and permissions management.

- **CUE for Configuration Validation:**
  - **Employing CUE:** CUE was introduced to validate configurations and ensure consistency across deployments. Its ability to import Kubernetes API specs and perform client-side validation offered a way to catch errors early in the development process, enhancing the reliability of deployments.

- **OCI Artifacts for Configuration Storage:**
  - **Utilizing OCI Artifacts:** Transitioning from Git to OCI artifacts for storing Kubernetes configurations was proposed as a solution to remove Git as a production dependency. This approach leverages container registries for storing and retrieving configurations, benefiting from their inherent scalability and availability.

- **Timony for Template Generation:**
  - **Introduction of Timony:** Stefan Prodan discussed Timony, a tool designed to generate Kubernetes configurations using CUE. Timony aims to simplify template creation by providing a type-safe, modular approach to defining Kubernetes resources, reducing the complexity associated with managing large-scale deployments.

- **Best Practices and Innovations:**
  - **Blueprints for Deployment Instructions:** Deployments are structured around blueprints, which define how workloads should be deployed across various clusters. These blueprints facilitate the reuse of configurations and streamline the deployment process.
  - **Variable Substitution and Config Maps:** Flux's variable substitution feature was highlighted as a flexible way to customize deployments per cluster, allowing for granular control over deployment parameters.

- **Future Directions:**
  - **Timony Development:** Future work includes stabilizing Timony's API, promoting its Flux distribution for edge deployments, and potentially developing a Timony controller for Kubernetes, aiming to further streamline the deployment process.

- **Closing Thoughts:**
  - The presentation underscored the importance of efficient configuration management in large-scale Kubernetes environments. Through the integration of Flux, CUE, OCI artifacts, and the innovative use of Timony, the speakers shared valuable strategies for managing complexity and improving deployment reliability.

<iframe width="720" height="720" src="https://www.youtube.com/embed/tClsqnZMN6I?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="eBPF’s Abilities and Limitations: The Truth - Liz Rice &amp; John Fastabend, Isovalent" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 16. eBPF’s Abilities and Limitations: The Truth - Liz Rice & John Fastabend, Isovalent

Liz Rice and John Fastabend's presentation delves into the capabilities and perceived limitations of eBPF (Extended Berkeley Packet Filter), a revolutionary technology that enhances the flexibility and security of the Linux kernel without altering its code. The session aims to demystify eBPF's potential and limitations through technical insights and demonstrations, including an eBPF-based implementation of Conway's Game of Life. Here are the key takeaways:

- **eBPF Overview:**
  - **Functionality:** eBPF allows for the execution of custom programs within the kernel, offering the ability to modify kernel behavior dynamically. This capability is crucial for tasks involving hardware access, network communication, and memory management.

- **Misconceptions Addressed:**
  - **Complex Processing:** Contrary to beliefs that eBPF cannot handle complex layer 7 processing, the speakers argue that eBPF's design and continuous evolution enable it to manage complex tasks effectively.
  - **Turing Completeness:** The discussion touched on Turing completeness, explaining that while eBPF is not Turing complete due to its design to ensure safety and prevent indefinite execution, it still can process complex and varied tasks within its operational constraints.

- **Verifier and Limitations:**
  - **Safety Mechanisms:** The eBPF verifier plays a crucial role in ensuring program safety by preventing unauthorized memory access and ensuring that eBPF programs terminate or release the CPU to avoid monopolizing system resources.
  - **Instruction Limits:** Initially capped at 4,096 instructions, eBPF programs can now contain up to 1 million instructions, vastly expanding their complexity and potential applications.

- **Looping and Function Calls:**
  - **Enhanced Capabilities:** Recent kernel versions have introduced support for bounded loops and function calls within eBPF programs, allowing for more sophisticated and efficient program structures.

- **Memory Allocation and Execution:**
  - eBPF's design includes mechanisms for allocating and managing memory efficiently, supporting large-scale and complex operations within the kernel space.

- **Demonstration - Game of Life:**
  - **Practical Example:** A live demonstration of Conway's Game of Life running as an eBPF program showcased eBPF's ability to execute complex algorithms. This example highlighted eBPF's growth beyond its initial networking-focused applications to more generalized computing tasks.

- **Future of eBPF:**
  - **Continuous Evolution:** eBPF is under active development, with the community working towards enhancing its functionality, usability, and scope. The speakers emphasized the importance of community engagement and contributions to driving eBPF forward.

- **eBPF's Impact on Infrastructure Tools:**
  - **Real-world Applications:** Projects like Cilium and Tetragon utilize eBPF for networking, security, and observability, underscoring eBPF's utility in creating powerful infrastructure tools.

In conclusion, Liz Rice and John Fastabend's session challenges misconceptions about eBPF's limitations, illustrating its potential through technical explanation and a creative demonstration. As eBPF continues to evolve, its applications in cloud-native technologies are expected to expand, further integrating eBPF into the fabric of modern computing environments.

<iframe width="720" height="720" src="https://www.youtube.com/embed/vLc-nCwojIM?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Squeeze Your K8s: How We Adopt Time-Series Forecasting Models in FinOps... Irvin Lim &amp; Nicholas Kwan" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 17.Squeeze Your K8s: How We Adopt Time-Series Forecasting Models in FinOps Practices by Irvin Lim &amp; Nicholas Kwan

Irvin Lim and Nicholas Kwan's presentation delves into leveraging time-series forecasting to enhance financial operations (FinOps) efficiency in Kubernetes (K8s) environments. They discuss their journey from initial node-level optimizations to comprehensive fleet management, aiming to reduce costs while supporting an ever-growing container workload. Here are the key insights:

- **Challenges at Scale:** Managing over 100,000 pods across thousands of nodes worldwide presents significant cost implications. The key challenge is optimizing resource use to support this scale without incurring prohibitive expenses.

- **Resource Utilization Patterns:** Observation revealed common cases of resource wastage, particularly during off-peak hours and campaign events. These patterns indicated opportunities for resource optimization.

- **Adopting Time-Series Forecasting:** The approach utilizes time-series forecasting to predict resource utilization, enabling more efficient resource allocation and scaling. Short-term forecasting smoothes out immediate fluctuations to minimize unnecessary evictions, while long-term forecasting predicts usage patterns to make informed scheduling decisions.

- **Implementing Extended Resources:** By monitoring unused node resources, they reclaimed and advertised these through extended resources, allowing for additional workload placements beyond allocated capacities.

- **Batch Services:** They introduced a category for batch services that could utilize reclaimed resources, adjusting CPU weights and utilizing Linux kernel features to ensure online services retain priority.

- **Forecasting Techniques:** Utilizing exponentially weighted moving averages (EWMA) for short-term forecasts and Fourier transforms for long-term forecasts, they smoothed and predicted resource utilization patterns. This allowed for more precise scaling and resource allocation strategies.

- **Architecture for Forecasting Implementation:** The architecture involved collecting metrics via Prometheus, processing data through Spark pipelines, and integrating forecasts into Kubernetes scheduling decisions. This system enables more effective resource utilization across their Kubernetes fleet.

- **Challenges and Considerations:** The approach required careful management to avoid excessive evictions or resource contention. Additionally, forecasting models needed to balance complexity and accuracy against performance and computational costs.

- **Future Directions and Improvements:** Exploring more sophisticated forecasting models, including machine learning and deep learning approaches, could offer improved accuracy and responsiveness to trends and anomalies. However, these models present their own set of challenges, such as training costs and the need for fine-tuning.

- **Impact and Results:** The adoption of time-series forecasting in FinOps has led to more efficient resource use, reduced costs, and supported the scaling of Shopee's Kubernetes infrastructure. The techniques discussed offer valuable insights into managing large-scale Kubernetes environments more effectively.

<iframe width="720" height="720" src="https://www.youtube.com/embed/46dGFsIxCLA?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="KubeDDR! a Dance of Predictive Scoring with MLOps, Step by Step - Leigh Capili &amp; Annie Talvasto" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 18. KubeDDR! A Dance of Predictive Scoring with MLOps, Step by Step - Leigh Capili & Annie Talvasto

Leigh Capili and Annie Talvasto's presentation on "KubeDDR! A Dance of Predictive Scoring with MLOps, Step by Step" offers an intriguing blend of machine learning operations (MLOps), Kubernetes (K8s), and the interactive game Dance Dance Revolution (DDR). Through a playful yet informative exploration, they demonstrate how predictive scoring models can be applied within the gaming and sports context, specifically focusing on DDR. Here are the detailed insights:

- **Combining Esports and Physical Activity:** DDR represents a unique intersection of esports and physical activity. By integrating computer vision and predictive scoring, the speakers showcase how technological advancements can enhance player interaction and game analysis.

- **Computer Vision for Game Interaction:** Using computer vision, the speakers describe a method for detecting when a DDR pad is occupied, highlighting the potential for real-time game interaction and scoring.

- **Predictive Scoring with OCR:** Optical Character Recognition (OCR) technology is leveraged to analyze DDR gameplay, focusing on detecting scores from the game display. This approach exemplifies how predictive scoring can be employed to dynamically evaluate player performance.

- **MLOps for Deploying Models:** The challenges of deploying machine learning models into production environments are addressed. MLOps practices, including containerization with Docker and orchestration with Kubernetes, are discussed as solutions to streamline the deployment process.

- **Practical Demonstration:** A live demonstration, involving DDR gameplay, computer vision, and OCR, vividly illustrates the application of the discussed technologies. Despite background noise and visual interference challenges, the demo provides a real-world example of predictive scoring in action.

- **The Path to Production:** Highlighting the hurdles of bringing machine learning models to production, the presentation acknowledges the complexity of MLOps and the necessity of overcoming technical challenges to achieve operational efficiency.

- **Resources and Further Reading:** Attendees are directed to additional resources for diving deeper into computer vision, MLOps, and the specifics of implementing similar projects. These resources serve as a springboard for further exploration and development in the field.

- **Future Directions:** The presentation hints at the broader applicability of the discussed technologies beyond gaming, suggesting potential uses in sports analytics, training, and even predictive modeling for event outcomes.

<iframe width="720" height="720" src="https://www.youtube.com/embed/4SqDRokC7ac?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Fluent Bit v3: Unified Layer for Logs, Metrics and Traces - Eduardo Silva, Calyptia" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 19. Fluent Bit v3: Unified Layer for Logs, Metrics and Traces - Eduardo Silva, Calyptia

Eduardo Silva’s presentation on Fluent Bit version 3 dives deep into the latest enhancements and features of Fluent Bit, a CNCF graduated project renowned for its efficiency in processing logs, metrics, and traces. Here's a concise overview of the key points discussed:

- **Background and Evolution:**
  - Fluent Bit has matured significantly since its inception, expanding its capabilities from logs to metrics and now traces. Originating from a need to manage telemetry data efficiently, Fluent Bit has evolved into a critical tool within the CNCF ecosystem, alongside Kubernetes and OpenTelemetry.

- **Adoption and Community Growth:**
  - The project’s success is highlighted by its wide adoption and active community contribution, underpinning its status as a trusted and sustainable solution for telemetry data management.

- **Unified Data Processing:**
  - Fluent Bit's architecture enables unified processing of logs, metrics, and traces, providing a cohesive solution for telemetry data. It abstracts data types to facilitate agnostic processing and flexible backend integrations.

- **Version 3 Enhancements:**
  - Key features introduced in Fluent Bit v3 include HTTP/2 support, a new serialization format for improved performance, and several powerful processors such as Content Modifier and SQL Processor, enhancing data manipulation capabilities.

- **Content Modifier Processor:**
  - This new processor supports a wide range of actions (e.g., insert, upsert, delete, rename) on logs and traces, demonstrating Fluent Bit’s flexibility in data handling.

- **Metrics Selector Processor:**
  - The Metrics Selector allows for the inclusion or exclusion of specific metrics by name, optimizing the telemetry data sent to backends.

- **SQL Processor:**
  - A significant addition, the SQL Processor, offers SQL-like querying capabilities for logs, enabling sophisticated data selection and transformation without custom scripting.

- **OpenTelemetry Integration:**
  - Fluent Bit ensures compatibility with OpenTelemetry, supporting both ingestion and export of telemetry data in OpenTelemetry formats, solidifying its role in modern observability stacks.

- **Performance and Scalability:**
  - The introduction of multi-threading and coroutine support in recent versions addresses scalability challenges, enabling efficient data processing and transmission even in high-volume environments.

- **Future Directions:**
  - Continuous innovation, with plans for expanded metrics capabilities, deeper integration with cloud-native ecosystems, and enhancements to the Fluent ecosystem, promise to further elevate Fluent Bit's utility.

- **Community-Driven Development:**
  - Silva emphasizes the importance of community feedback and contributions in shaping Fluent Bit's roadmap, reflecting the project’s commitment to addressing real-world challenges in observability.

<iframe width="720" height="720" src="https://www.youtube.com/embed/5scIQkclPfk?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Kubernetes Security Blind Spot: Misconfigured System Pods - Shaul Ben Hai, Palo Alto Networks" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 20.Kubernetes Security Blind Spot: Misconfigured System Pods - Shaul Ben Hai, Palo Alto Networks

Shaul Ben Hai from Palo Alto Networks addresses a significant vulnerability in Kubernetes: the security blind spot created by misconfigured system pods. This presentation delves into the nuances of container escape and second-stage attacks in Kubernetes, emphasizing the critical role system pods play in both the functionality and security of Kubernetes clusters.

**Container Escape and Second-Stage Attacks:**
- Containers, despite their packaging and deployment benefits, are not foolproof security boundaries due to shared kernels. Container escapes are likely to continue, highlighting the importance of understanding their impact on Kubernetes, where they can compromise nodes or even entire clusters.

**System Pods - Privilege vs. Risk:**
- System pods, essential for cluster operation, pose a unique security challenge due to their elevated privileges. These pods execute critical tasks and maintain cluster functionality but their elevated access makes them a high-value target for attackers.

**Real-World Misconfigurations and Vulnerabilities:**
- The talk presents a case study combining two default misconfigurations in Google Kubernetes Engine (GKE), demonstrating how these can be exploited to escalate privileges to cluster admin level.

**Security Recommendations and Best Practices:**
- Key recommendations include minimizing pod privileges, limiting network access, and proactive monitoring to mitigate the inherent risks associated with system pods.

**Insights based on numbers:**
- The analysis emphasizes that while containers offer deployment advantages, their security implications, especially in the context of Kubernetes, cannot be overlooked. The dual nature of system pods as both essential and potentially vulnerable highlights the complex security landscape of Kubernetes clusters.

<iframe width="720" height="720" src="https://www.youtube.com/embed/8Zwftqf8g8w?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Keeping Kubernetes Safe: The Lowdown on Locked Namespaces - Marco De Benedictis, ControlPlane" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 21.Keeping Kubernetes Safe: The Lowdown on Locked Namespaces - Marco De Benedictis, ControlPlane

Educational summary of [Keeping Kubernetes Safe: The Lowdown on Locked Namespaces - Marco De Benedictis, ControlPlane](https://youtu.be/8Zwftqf8g8w) by [Video Summarizer](https://chat.openai.com/g/g-GvcYCKPIH-video-summarizer-ai):

Marco De Benedictis from ControlPlane discusses the critical role of namespaces in Kubernetes security, highlighting how misconfigured namespaces can expose Kubernetes clusters to various security threats. This presentation not only identifies common misconfigurations and their implications but also offers practical mitigation strategies.

**Namespace Basics and Security Implications:**
- Namespaces are not inherently security features but play a crucial role in organizing resources and influencing the security posture of a Kubernetes cluster.
- Misconfigurations at the namespace level can significantly impact security, affecting everything from role-based access control (RBAC) to resource management and network policies.

**Role-Based Access Control (RBAC):**
- Namespaces are essential for enforcing authorization policies within a cluster, allowing for fine-grained access control to resources.

**Networking and Namespaces:**
- Several Kubernetes networking features rely on namespace configurations, including DNS resolution, cross-namespace communication, and network policies. Misconfigured namespaces can lead to unauthorized access and potential data exfiltration.

**Security Context and Pod Security Standards:**
- Pod security standards, which replace Pod Security Policies (PSPs), are defined at the namespace level. Misconfigurations can lead to lax security contexts, allowing for the execution of privileged operations.

**Resource Management:**
- Kubernetes features like resource quotas and limit ranges are namespace-bound and crucial for preventing resource exhaustion attacks by noisy neighbors or malicious actors.

**Mitigations and Best Practices:**
- Least privilege RBAC: Ensure access to namespace objects is minimized, and audit changes regularly.
- Use immutable labels: Apply immutable labels to prevent unauthorized changes by attackers.
- Policy engines: Implement policies using tools like Kyverno to enforce namespace naming conventions, restrict label modifications, and prevent the creation of namespaces with names mimicking top-level domains or system namespaces.

**Insights based on numbers:**
- The presentation underscores the importance of namespaces in Kubernetes security architecture, showing how they intersect with multiple security mechanisms. Despite not providing isolation, namespaces serve as a fundamental aspect of security feature configurations, underscoring the criticality of proper management and configuration to safeguard Kubernetes environments.

<iframe width="720" height="720" src="https://www.youtube.com/embed/9A1mU-EbXrE?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Precision Matters: Scheduling GPU Workloads on Kubernetes - Amit Kumar &amp; Gaurav Kumar, Uber" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 22.Precision Matters: Scheduling GPU Workloads on Kubernetes - Amit Kumar &amp; Gaurav Kumar, Uber

Amit Kumar and Gaurav Kumar from Uber discuss the complexities and solutions related to scheduling GPU workloads on Kubernetes. This session delves into the challenges faced by Uber’s Compute Platform team in efficiently utilizing GPU resources for AI/ML workloads.

**Challenges in GPU Workload Scheduling:**
- **Heterogeneous Clusters:** Ensuring that GPU resources are used exclusively for GPU workloads, preventing CPU workloads from occupying GPU nodes.
- **Multiple GPU SKUs:** Matching workloads with appropriate GPU types to ensure optimal performance and cost efficiency.
- **Resource Fragmentation:** Minimizing resource fragmentation to ensure that larger GPU workloads can be accommodated.

**Solutions and Implementations:**
- **Node Labeling:** Labeling nodes based on GPU presence and type to guide scheduling decisions.
- **Filter Plugins:** Developing custom Kubernetes scheduler plugins to prevent non-GPU workloads from being scheduled on GPU nodes and to ensure that workloads are matched with appropriate GPU SKUs.
- **Bin Packing Strategy:** Adjusting scheduling strategies to pack workloads more densely and reduce fragmentation.

**Benefits and Outcomes:**
- Enhanced **resource utilization** and **cost savings**, estimated at half a million dollars annually, by ensuring workloads are scheduled on appropriate GPU types.
- **Improved model accuracy** by matching workloads with the best-suited GPU SKUs.

**Common Pitfalls and Solutions:**
- **Device Plugin Conflicts:** Addressed by excluding system pods, including the NVIDIA device plugin, from custom filter plugin logic.
- **Over-requesting Resources:** Implementing admission controls to prevent scheduling pods that request more GPUs than available on any single node.
- **Privileged Containers:** Prohibiting privileged containers that can bypass resource limits and access all node GPUs.

**Future Directions:**
- **Fractional GPUs:** Investigating the use of NVIDIA Multi-Instance GPU (MIG) to allow workloads to share GPU resources more effectively.
- **Support for Multiple GPU Vendors:** Exploring the inclusion of GPUs from Intel and AMD to diversify the hardware and optimize cost and performance.

**Insights based on numbers:**
- Uber’s GPU utilization never exceeds 80% to ensure reliability and performance. The Compute Platform team continuously works on improving utilization rates for both CPU and GPU resources.

<iframe width="720" height="720" src="https://www.youtube.com/embed/ABbds5-zSqk?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Leveraging OCI 1.1 for Enhanced SBOM Integration and Vulnerability Scanning in Harbor" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 23.Leveraging OCI 1.1 for Enhanced SBOM Integration and Vulnerability Scanning in Harbor

This presentation focuses on the integration of the Open Container Initiative (OCI) 1.1 specification with Harbor, a Cloud Native Computing Foundation (CNCF) graduated project, to enhance the generation and management of Software Bills of Materials (SBOMs) and vulnerability scanning. The talk, delivered by experts in the field, outlines how Harbor leverages OCI 1.1 to attach SBOMs to container images, thereby improving vulnerability scanning capabilities and software supply chain security.

**Key Points Discussed:**

- **SBOM Importance:** SBOMs provide a comprehensive inventory of all components within a software artifact, enabling better security and compliance management. They are crucial for identifying vulnerabilities quickly and are encouraged by recent executive orders and security best practices.

- **OCI 1.1 Specification:** OCI 1.1 introduces the ability to link artifacts within OCI-compliant registries, allowing for SBOMs, signatures, and other security artifacts to be associated with container images. This significantly enhances the traceability and management of components across the software supply chain.

- **Harbor's Role:** Harbor, an open-source artifact registry, has adopted OCI 1.1 to allow users to attach SBOMs and other security documents to container images. Harbor’s plugable scanner framework also facilitates the integration with various vulnerability scanners, including Trivy, a comprehensive security scanner developed by Aqua Security.

- **Integration with Trivy:** The presentation showcases how Trivy is used within Harbor to generate and attach SBOMs to container images automatically. Trivy supports generating SBOMs in popular formats like SPDX and CycloneDX, which can be attached to images in Harbor as "accessories," enhancing the visibility of the software components and their vulnerabilities.

- **Automated SBOM Generation:** Harbor's configuration can be adjusted to automatically generate SBOMs upon image push, streamlining the process for developers and ensuring that all artifacts in Harbor have corresponding SBOMs for security analysis.

- **Vulnerability Scanning Enhancements:** With the integration of OCI 1.1 and SBOMs, Harbor can provide more efficient vulnerability scanning by analyzing the SBOM directly rather than the container image itself. This approach can lead to faster scans and more accurate detection of vulnerabilities applicable to the software components listed in the SBOM.

<iframe width="720" height="720" src="https://www.youtube.com/embed/B0Xm2kfU-yA?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="IAM Confused: Analyzing 8 Identity Breach Incidents - Maya Levine, Sysdig" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 24.IAM Confused: Analyzing 8 Identity Breach Incidents - Maya Levine, Sysdig

Maya Levine from Sysdig offers a comprehensive analysis of eight real-world identity breach incidents, shedding light on the crucial importance of robust identity and access management (IAM) in cloud security. The presentation underscores how IAM failures are a common thread in many security breaches, illustrating through real incidents the tactics used by attackers and providing actionable insights to fortify defenses.

**Key Insights from Identity Breach Incidents:**

- **Widespread IAM Confusion:** Levine emphasizes that confusion around IAM is understandable due to its complexity and rapid evolution. IAM is likened to the cloud's perimeter, with mismanagement of permissions, secrets, and identities being exploited in many breaches.

- **The Over-Permission Problem:** Research indicates that a staggering 98% of granted permissions are unused, presenting a significant attack surface. The talk stresses the necessity of adhering to the principle of least privilege and tailoring permissions closely to actual needs.

- **Human and Machine Identity Challenges:** Mistakes in IAM can stem from unclear ownership between DevOps and security teams, leading to vulnerabilities. Levine points out that attackers are adept at secret harvesting and can exploit even seemingly minor oversights, like case sensitivity in IAM policies.

- **Real-World Breach Analysis:** The presentation meticulously dissects several breach incidents, including:
    - A Kubernetes cluster breach where Jupiter notebook containers were exploited.
    - Attackers leveraging IAM misconfigurations and default roles to escalate privileges.
    - Social engineering attacks manipulating MFA and phishing to gain unauthorized access.
    - Hardcoded credentials in scripts providing attackers with a "golden key" to extensive privileges.
    - Misconfigured Kubernetes API servers allowing unauthorized access and persistence establishment.
    - The use of stolen VPN credentials to infiltrate and exfiltrate sensitive information.

**Strategies for Enhancing IAM Security:**

- **Strict Permission Management:** Implement a least-privilege model and regularly audit permissions to ensure they align with actual usage.

- **Clear Ownership and Accountability:** Define and communicate responsibilities for IAM posture between DevOps, IT, and security teams to prevent gaps and oversights.

- **Secrets Management:** Employ a centralized secrets management system to prevent credentials from being hardcoded or stored insecurely.

- **Education and Training:** Increase awareness and training for employees to recognize and resist social engineering attempts.

- **Advanced MFA Strategies:** Beyond enabling MFA, organizations should restrict MFA registrations to trusted locations and devices, limit the number of devices that can receive MFA requests, and alert on login attempts from unusual locations.

- **Invest in IAM Tools and Processes:** Utilize Cloud Infrastructure Entitlement Management (CIEM) tools to detect excessive permissions and improve identity hygiene. Ensure network segmentation to hinder lateral movement and deploy Cloud Detection and Response (CDR) for real-time threat detection.

<iframe width="720" height="720" src="https://www.youtube.com/embed/Cr2Kwht3lqY?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="You Shall Not Pass! Unless You Are GUAC Verified - Parth Patel, Kusari &amp; Dejan Bosanac, Red Hat" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 25.You Shall Not Pass! Unless You Are GUAC Verified - Parth Patel, Kusari &amp; Dejan Bosanac, Red Hat

Parth Patel of Kusari and Dejan Bosanac of Red Hat delve into the criticality of software supply chain security, particularly focusing on the GUAC (Graph for Understanding Artifact Composition) framework. This presentation showcases how GUAC, alongside Kusari and other open-source projects, enhances the security and trustworthiness of software artifacts through comprehensive verification processes.

**Key Highlights:**

- **Software Supply Chain Vulnerabilities:** The talk begins with an overview of the diverse threats plaguing the software supply chain, ranging from source code to build and packaging vulnerabilities, emphasizing the necessity of vigilant management across all stages.

- **Introduction to GUAC:** GUAC serves as a central repository that aggregates data from various sources, including SBOMs (Software Bill of Materials), vulnerability databases, and build attestations. It represents these relationships within a graph database, facilitating a more nuanced understanding of artifact composition and potential security risks.

- **Vulnerability Exploitability Exchange (VEX):** A crucial part of GUAC's utility lies in its ability to filter out the noise of omnipresent vulnerabilities by leveraging VEX files. VEX files allow maintainers to indicate whether specific vulnerabilities affect their project, thus enabling targeted and efficient security measures.

- **Demonstration of GUAC in Action:** The presentation includes a live demo where GUAC, integrated with Kubernetes through Open Policy Agent (OPA) and Gatekeeper, enforces deployment policies based on the security posture of container images. It illustrates GUAC's ability to reject or accept deployments based on the presence of SBOMs, build attestations, and identified vulnerabilities.

- **Policy Enforcement and Automation:** By incorporating policy checks directly into the Kubernetes deployment process, GUAC empowers organizations to automate security decisions. It supports a proactive approach to software security, ensuring that only verified artifacts are deployed.

- **Collaboration and Open Source Contribution:** Patel and Bosanac stress the importance of community involvement in the ongoing development and refinement of GUAC and related projects. They invite contributions to address the numerous challenges in supply chain security and to help evolve GUAC's capabilities.

<iframe width="720" height="720" src="https://www.youtube.com/embed/H0Cdf--4uq8?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Search at Shopify: Highly Available Platform for Data Resilience and Compliance - Leila Vayghan" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 26.Search at Shopify: Highly Available Platform for Data Resilience and Compliance - Leila Vayghan

Leila Vayghan, an infrastructure engineer at Shopify, provides a comprehensive overview of Shopify's search platform, detailing the architecture, technologies, and strategies employed to ensure high availability, scalability, and global access. The presentation focuses on how Shopify hosts its search infrastructure on Kubernetes, managing data resilience and compliance effectively.

**Shopify's Search Infrastructure Overview:**
- Shopify, a major cloud-based commerce platform, handles immense volumes of data and requests, especially during peak events like Black Friday Cyber Monday (BFCM).
- The search functionality is crucial for both merchants and buyers, utilizing Elasticsearch as a secondary data store for fast search capabilities, full-text search, and data aggregations.

**Technological Foundation:**
- **Elasticsearch:** A distributed search and analytics engine known for its scalability and fault tolerance, essential for e-commerce functionalities.
- **Kafka:** An open-source distributed streaming platform used at Shopify for building real-time data pipelines and streaming applications, facilitating the movement of data from SQL databases to Elasticsearch.

**Shopify's Data Pipeline:**
- The data pipeline, consisting of Kafka topics and consumers, processes records from Shopify's SQL databases, pushing updates to Elasticsearch in real time.
- Shopify maintains a "Search Platform" team, dedicated to developing and maintaining the infrastructure that powers search functionalities across the platform.

**High Availability and Scalability:**
- The presentation underscores the importance of high availability and redundancy, with Elasticsearch providing built-in fault tolerance through shard replication across multiple nodes.
- Shopify's custom Kubernetes controller plays a pivotal role in managing Elasticsearch clusters, ensuring data redundancy and handling scalability needs, including automatic storage scaling.

**Globalization and Data Locality:**
- Post-COVID-19, Shopify expanded its operations globally to reduce latency and comply with data jurisdiction requirements, necessitating the search infrastructure to be closer to clients worldwide.
- Shopify manages over 100 distinct Elasticsearch clusters across the globe, handling more than 3 petabytes of data and ensuring that search functionalities are optimized for merchants and buyers in various regions.

**Key Takeaways:**
- **Custom Kubernetes Controller:** Shopify developed a custom controller to manage Elasticsearch clusters on Kubernetes, allowing for flexible, scalable, and highly available search services.
- **Data Pipeline Efficiency:** Through the integration of Kafka for real-time data processing and Elasticsearch for search functionalities, Shopify ensures a seamless and efficient data pipeline.
- **Global Reach and Compliance:** The search platform's expansion and adaptability to various regions underline Shopify's commitment to providing fast, reliable, and compliant search experiences to its global user base.

<iframe width="720" height="720" src="https://www.youtube.com/embed/IIxQHQHOXs4?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Kubernetes Is FINALLY Removing in-Tree Cloud Providers - Bridget Kromhout &amp; Chris Privitere" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 27.Kubernetes Is FINALLY Removing in-Tree Cloud Providers - Bridget Kromhout &amp; Chris Privitere

Bridget Kromhout and Chris Privitere highlight a significant shift in Kubernetes' architecture during KubeCon EU 2024, focusing on the transition from in-tree to out-of-tree cloud providers. This change aims to streamline Kubernetes' core codebase, enhancing its maintainability and adaptability across various cloud environments.

**Overview of the Transition:**
- **In-Tree vs. Out-of-Tree:** Originally, Kubernetes integrated cloud provider-specific code directly into its core (in-tree), leading to a bloated and complex codebase. The move to out-of-tree means cloud provider code is managed externally, facilitating easier updates and customizations.

**Implications for Kubernetes Users:**
- **Increased Flexibility:** Users can now choose and upgrade cloud provider integrations independently of Kubernetes' core releases, allowing for more agile adaptation to new cloud features and bug fixes.
- **Improved Code Maintainability:** Removing provider-specific code from Kubernetes' core reduces its complexity, making the ecosystem more accessible for contributors and simplifying the development of new features.

**Migration Challenges and Solutions:**
- **Transition Complexity:** The shift requires significant effort from both the Kubernetes community and cloud providers to ensure a smooth migration for users. Detailed documentation and clear migration paths have been crucial in this process.
- **Testing and Coordination:** Extensive testing is required to ensure that out-of-tree providers maintain compatibility with Kubernetes. Collaboration among SIGs (Special Interest Groups) has been vital for addressing test failures and race conditions discovered during the transition.

**Community Involvement:**
- **Contributions Welcome:** The Kubernetes community encourages contributions to help with the transition, especially in testing and documentation. Bridget and Chris emphasize the importance of community engagement in SIG meetings and Slack channels to guide the evolution of cloud provider integrations.

**Actionable Takeaways:**
- **Check Cloud Provider Status:** Users of cloud providers previously integrated in-tree should verify their migration status to out-of-tree versions to avoid disruptions.
- **Get Involved:** There are numerous opportunities to contribute to the transition, from code contributions to participation in SIG Cloud Provider discussions.

<iframe width="720" height="720" src="https://www.youtube.com/embed/JJ4sYWxcRok?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Sponsored Keynote: Operating AI Services on Cloud Native Technologies" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 28.Sponsored Keynote: Operating AI Services on Cloud Native Technologies

This keynote discusses the integration of AI services with cloud-native technologies, highlighting Oracle's journey and contributions to open-source projects and the Cloud Native Computing Foundation (CNCF). It emphasizes the role of Kubernetes in enhancing AI service performance and introduces Oracle's initiatives to support AI development with CPUs and eco-friendly technologies.

**Key Points:**
- **Oracle's Open Source Contribution:** Oracle is a major contributor to over 500 open-source projects, including significant contributions to the Linux Kernel and OpenJDK.
- **Cloud Native and Kubernetes Adoption:** Oracle Cloud Infrastructure (OCI) is built on open standards, emphasizing no vendor lock-in and supporting multicloud strategies. OCI and its AI services leverage Kubernetes for orchestration, significantly improving performance.
- **Innovation in AI with CPUs and ARM:** Oracle explores using CPUs for AI inferencing workloads, offering a cost-effective and widely available alternative to GPUs. This approach has been extended to ARM CPUs, focusing on eco-friendly AI computing.
- **Support for CNCF Projects:** Oracle announced a donation of 3 million credits for ARM to CNCF projects, encouraging the use of eco-friendly computing resources for AI and cloud-native applications.

<iframe width="720" height="720" src="https://www.youtube.com/embed/T_ABA749zro?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="It&#39;s Not Just About SBOMs: Perspectives on Cloud Native Supply Chain Security" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 29.It's Not Just About SBOMs: Perspectives on Cloud Native Supply Chain Security

This panel discussion delves into the multifaceted challenges and solutions associated with securing the software supply chain in cloud-native environments, going beyond the scope of Software Bill of Materials (SBOMs) to explore broader security considerations.

**Critical Insights:**
- **Beyond SBOMs:** While SBOMs play a crucial role in enhancing transparency within the software supply chain, they represent only a component of a comprehensive security strategy. Panelists emphasize the importance of a holistic approach that includes secure software development, storage, distribution, and consumption practices.
- **Comprehensive Supply Chain Security:** Security within the software development lifecycle (SDLC) involves preventing vulnerabilities from being exploited and ensuring the integrity of software from production to consumption. The discussion highlighted the need for robust mechanisms to safeguard against threats at various stages of the SDLC, including code commits, repository compromises, and dependency management.
- **Significance of Attestations:** The panelists advocate for the generation and management of attestations throughout the supply chain. These attestations, which include details about the secure creation and handling of software artifacts, are crucial for verifying the trustworthiness of software components.
- **Role of Tools and Frameworks:** Various tools and frameworks are identified as vital for supporting supply chain security efforts. Projects such as SLSA (Supply chain Levels for Software Artifacts), Scorecards, and Witness focus on securing builds and repositories, while initiatives like TUF (The Update Framework) and InToto offer frameworks for securing software updates and providing end-to-end supply chain integrity.
- **Challenges and Opportunities:** The discussion acknowledges the complexity of supply chain security, pointing out challenges such as the variety of attack vectors, the need for extensive tooling to produce and consume SBOMs effectively, and the importance of establishing trusted identities within the supply chain.
- **Practical Steps Forward:** Panelists suggest starting with threat modeling of the software supply chain to identify vulnerabilities and prioritize security measures. They also stress the importance of continuous learning and incremental implementation of security practices to manage the complexity of supply chain security effectively.

<iframe width="720" height="720" src="https://www.youtube.com/embed/Tmo5EUsxLac?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Maximizing GPU Utilization Over Multi-Cluster: Challenges and Solutions for Cloud-Native AI Platform" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 30.Maximizing GPU Utilization Over Multi-Cluster: Challenges and Solutions for Cloud-Native AI Platform

This presentation by William Wan and H.M.H Kai explores strategies for maximizing GPU utilization across multi-cluster environments, addressing key challenges and offering solutions tailored for cloud-native AI platforms. They highlight the importance of efficient GPU management in the context of increasing AI demands and the complexities of distributed resources.

**Key Challenges:**
- **Resource Distribution:** GPUs are often dispersed across various locations, including different IDCs and cloud providers, complicating unified management.
- **Diverse GPU Models:** The presence of multiple GPU models and versions complicates resource sharing among different teams.
- **Access Time:** Obtaining a ready-for-production GPU cluster can take weeks due to high demand and limited availability.

**Proposed Solutions:**
- **Unified Platform:** The creation of a platform capable of managing GPU resources across different regions and cloud providers is crucial. Such a platform should standardize GPU computing power across generations and include an intelligent scheduler for resource allocation based on global resource visibility.
- **Use of Kubernetes and Volcano:** Leveraging Kubernetes and Volcano can offer a multi-cluster AI solution that addresses resource management and scheduling challenges. This approach enables uniform resource management and efficient job scheduling to enhance GPU utilization.
- **Job Abstraction for AI Workloads:** An effective job abstraction is essential to support various AI training frameworks. Volcano's job CRD (Custom Resource Definitions) provide a robust abstraction that caters to the unique characteristics of AI jobs, including their structure and resource allocation needs.
- **Resource Sharing Mechanisms:** Implementing resource sharing policies based on specific scenarios can optimize GPU resource usage within clusters. The proposed strategies include proportion-based scheduling for fair resource allocation and capacity scheduling for resource reservation and maximum capacity management.
- **Multi-Workload Deployment:** Deploying diverse workloads within a single cluster, such as combining big data workloads with microservices or training with inference workloads, can improve resource utilization. However, ensuring QoS (Quality of Service) for high-priority tasks is vital, necessitating kernel-level isolation and resource oversubscription to increase container density on nodes.
- **Multi-Cluster Management with Karmada:** Karmada offers a Kubernetes management system that allows for application deployment across multiple clusters, including in multi-cloud environments. This system supports cross-cluster failover, service discovery, and application migration, ensuring service continuity and efficient resource utilization.

<iframe width="720" height="720" src="https://www.youtube.com/embed/ZYsUbN6oT7Q?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Lessons Learned from Generating 100M SBOMs: Google’s Approach to SBOM Compliance" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 31.Lessons Learned from Generating 100M SBOMs: Google’s Approach to SBOM Compliance

This presentation, delivered by Isaac Hepworth and Brandon, delves into Google's extensive experience with generating Software Bill of Materials (SBOMs), spurred by the requirements of a US executive order to improve cybersecurity. It outlines the journey Google embarked on to understand, generate, and manage SBOMs for its vast array of products, shedding light on the challenges, strategies, and insights gained throughout the process.

**Google's SBOM Journey:**
- **Understanding the Requirement:** The US Executive Order 14028 necessitated software suppliers to the federal government to produce SBOMs. This led Google to scrutinize what SBOMs entail, how they should be structured, and what information they must include, with guidance from NTIA's minimum requirements for SBOMs.
- **Scope and Prioritization:** Identifying which Google products were in scope for SBOM generation was crucial. Prioritization was guided by NIST's definition of 'critical software', focusing on operational hygiene and discipline in Google's software supply chain.

**Technical Approach and Solutions:**
- **Generation:** The cornerstone of Google's strategy was the reliance on build tools for SBOM generation, ensuring accuracy and completeness. This approach was supplemented by specific tooling for ecosystems like Android and internal tools for a long tail of software, emphasizing the use of SPDX as the standard format.
- **Storage and Trustworthiness:** Google developed 'CYO', a supply chain integrity log, to gather and make software metadata usable, ensuring SBOMs' integrity through validation and signature. This ensured trustworthiness, a key factor for compliance and vulnerability management.
- **Retrieval and Mapping:** The retrieval process required navigating a complex supply chain graph to associate SBOMs with their respective software artifacts accurately. This process underscored the importance of generating SBOMs at every build step and maintaining an inventory mapping for product dependencies.

**Insights and Hot Takes:**
- **SBOMs are Not the End Goal:** The primary objective of the executive order is not the SBOMs themselves but catalyzing the industry towards better cybersecurity practices. SBOMs necessitate a baseline operational discipline that enhances overall security posture.
- **Challenges with SaaS Products:** SBOMs present a challenge for SaaS products due to the difficulty in defining a comprehensive list of dependencies for services that are continuously updated and integrated with various other services and infrastructures.
- **Potential for Broader Impact:** Google's experience with SBOMs has uncovered broader applications beyond compliance, such as using SBOMs for security investment decisions and vulnerability management.

<iframe width="720" height="720" src="https://www.youtube.com/embed/aAxl90o910g?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Living off the Land Techniques in Managed Kubernetes Clusters - Ronen Shustin &amp; Shay Berkovich" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 32.Living off the Land Techniques in Managed Kubernetes Clusters - Ronen Shustin &amp; Shay Berkovich

Ronen Shustin and Shay Berkovich delve into "Living Off the Land" (LotL) techniques within managed Kubernetes clusters, exploring how attackers utilize native tools and processes for malicious purposes. The presentation examines various methods attackers employ to exploit Kubernetes clusters, demonstrating the significance of understanding and mitigating these threats.

**Key Points and Demonstrations:**

- **LotL Techniques Defined:** LotL refers to the use of existing, legitimate tools within an environment for malicious activities. In the context of Kubernetes, this includes exploiting binaries, middleware workloads, and even core Kubernetes functionalities.

- **Increased Attack Surface in Managed Clusters:** Managed Kubernetes services come pre-loaded with numerous workloads and configurations. This complexity adds to the attack surface, making clusters more susceptible to LotL techniques.

- **Persistence through Node Problem Detector:** Demonstrating a persistence technique, the presenters showed how attackers could exploit the Node Problem Detector on GKE and AKS clusters. By replacing the Python script used by the Node Problem Detector with a malicious script, attackers achieve persistent access and command execution on the worker node.

- **Data Collection via Fluent Bit:** The use of Fluent Bit for log management in Kubernetes clusters was highlighted as a vector for data exfiltration. By modifying the Fluent Bit's configuration, attackers can redirect log data to an external server, compromising sensitive information.

- **Privilege Escalation through Misconfigurations:** The presentation covered how misconfigurations in service accounts and roles, particularly in clusters integrated with Azure AD for authentication, could lead to privilege escalation. By exploiting these configurations, attackers can gain unauthorized access to cluster resources.

- **Impact of Host Network Configuration:** Using host network configurations improperly can expose clusters to risks. The demonstration showcased how an attacker could capture tokens from the EKS Pod Identity Webhook by capturing network traffic, allowing unauthorized access to AWS resources.

**Mitigation and Recommendations:**

- **Awareness and Standardization:** Raising awareness about LotL techniques in Kubernetes and standardizing threat metrics to include these techniques are critical first steps in mitigation.

- **Transparency from CSPs:** Encouraging cloud service providers (CSPs) to disclose the components and configurations included in managed clusters can help users understand and manage their risk profiles better.

- **Community Collaboration:** The speakers proposed creating a project to collect and categorize Kubernetes-specific LotL techniques, akin to existing projects like GTFOBins, to facilitate community-driven solutions and awareness.

<iframe width="720" height="720" src="https://www.youtube.com/embed/gjl-lTF70HE?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Kubernetes MLSec: Securing AI in Space - Francesco Beltramini &amp; James Callaghan, ControlPlane" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 33. Kubernetes MLSec: Securing AI in Space - Francesco Beltramini &amp; James Callaghan, ControlPlane

Francesco Beltramini and James Callaghan from ControlPlane discuss the intricate details of securing AI and machine learning systems, particularly in the context of space technology and Kubernetes. This presentation addresses the multifaceted challenge of embedding security throughout the lifecycle of machine learning applications, from data collection to model deployment, within a Kubernetes environment.

**Introduction to AI Ecosystem:**
- **Levels of AI:** They categorize artificial intelligence into artificial narrow intelligence (weak AI), artificial general intelligence (strong AI), and artificial superintelligence, explaining the current state and theoretical futures of AI.
- **Machine Learning Focus:** The discussion zeroes in on machine learning (ML), particularly supervised learning, and its applicability in predictive analytics and classification tasks.

**Challenges in MLOps Security:**
- **Complex Ecosystem:** The AI/ML ecosystem's complexity, with its extensive dependencies and data flows, significantly complicates the security landscape.
- **Data Security:** Ensuring the integrity and confidentiality of data throughout its lifecycle is paramount, given its foundational role in ML.
- **Model Security:** Protecting the ML models against tampering and misuse while maintaining their reliability and effectiveness.
- **Infrastructure and Orchestration:** The underlying Kubernetes infrastructure and orchestration mechanisms must be secured to safeguard the overall ML operations.

**Threat Modeling and Security Mitigations:**
- **Eternal Knot of DevOps and DataML:** They illustrate how the merging of DevOps and DataML lifecycles creates an "Eternal Knot," emphasizing the need for an integrated approach to security—MLSecOps.
- **Threat Modeling:** Employing threat modeling to systematically identify and address vulnerabilities across different stages of the ML lifecycle, from data ingestion and model training to deployment and monitoring.
- **Security Controls:** The adoption of robust security controls, including digital signing of artifacts, role-based access control (RBAC) on Kubernetes resources, network segmentation, and stringent application of the principle of least privilege.

**Real-world Use Case - Satellite Telemetry:**
- **Application in Space:** Highlighting a use case involving satellite telemetry, they explore how machine learning can shift from reactive to proactive anomaly detection, significantly enhancing operational efficiency and safety in space missions.
- **Threats and Mitigations:** Discussing specific threats to each stage of the ML lifecycle in this context and proposing targeted security measures to mitigate these risks, emphasizing the importance of securing data pipelines and model integrity.

**AI-Specific Threat Categories:**
- **Model and System Threats:** Including model inversion and executable code injection, highlighting the unique challenges posed by AI systems.
- **Access, Privacy, and Compliance:** Addressing concerns around data privacy violations and regulatory compliance, crucial for maintaining user trust and legal conformity.
- **Monitoring and Response:** The need for advanced anomaly detection techniques to promptly identify and mitigate security incidents in AI systems.

**Key Takeaways:**
- **Comprehensive Security Approach:** MLSecOps requires a holistic security strategy that encompasses both traditional cybersecurity measures and AI-specific considerations.
- **Collaboration and Continuous Learning:** The evolving nature of AI technologies and threats necessitates ongoing collaboration among data scientists, ML engineers, security professionals, and contributions to community-driven initiatives like the Kubernetes TAG Security.

**Conclusion:**
Beltramini and Callaghan advocate for integrating security into every facet of the ML lifecycle within Kubernetes, highlighting the necessity of a collaborative approach to navigate the complex, evolving landscape of AI security. They encourage participation in community efforts to further secure ML applications and infrastructure.

<iframe width="720" height="720" src="https://www.youtube.com/embed/nZLz0o4duRs?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Building Container Images the Modern Way - Adrian Mouat, Chainguard" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 34.Building Container Images the Modern Way - Adrian Mouat, Chainguard

Adrian Mouat of Chainguard presents a comprehensive overview of modern tools and methodologies for building container images, emphasizing the need for simplicity, speed, and security. The talk navigates through various container image build tools, comparing their features, advantages, and challenges.

**Container Image Evolution:**
- **Historical Context:** Mouat begins with a reflection on how Docker revolutionized software packaging, comparing it to previous practices using tarballs and package managers. This set the stage for the necessity of more efficient, secure, and manageable container image building tools.

**Modern Build Tools and Practices:**
- **Minimal Images:** The presentation underscores the importance of building minimal images, which contain only the application and its immediate dependencies, to mitigate security risks and reduce image size.
- **Multi-Stage Builds:** Mouat highlights the effectiveness of multi-stage Docker builds in creating minimal runtime images, particularly when combined with base images designed for minimalism, such as those provided by Chainguard.

**Exploration of Build Tools:**
- **KO for Go:** For Go applications, KO emerges as a simple and effective tool, capable of generating minimal container images without configuration files.
- **Bazel and Dagger:** These tools offer powerful features beyond just building container images, catering to broader software build and CI/CD pipeline needs. However, their complexity and learning curve may not suit all developers.
- **Apko and Wolfi:** Apko, used by Chainguard for building distroless images, is praised for its simplicity and reproducibility, using Wolfi as a minimal Linux distribution.
- **Build Packs and BuildKit:** Mouat reviews Build Packs and BuildKit, noting their potential but also pointing out some limitations and challenges, particularly in achieving minimal and secure images.
- **NYX:** Despite its theoretical advantages for reproducibility and minimalism, Mouat expresses difficulty in navigating NYX's complexity and lack of a clear, standard approach to building container images.

**Key Recommendations:**
- **Use Case Specificity:** Mouat advises selecting build tools based on specific project needs, ecosystem (e.g., Go, Java), and the scale of deployment.
- **Distroless and Minimal Images:** For most users, Mouat recommends multi-stage Docker builds using distroless or minimal base images to ensure security and efficiency.
- **Community and Ecosystem:** The presentation also touches on the importance of community contributions and using tools supported by active ecosystems to ensure long-term viability and support.

**Conclusion:**
Mouat's talk delivers a rich comparison of container image build tools, shedding light on the evolving landscape and guiding developers towards making informed choices based on their specific needs and project contexts. He advocates for a balanced approach that prioritizes minimalism, reproducibility, and security in building container images.

<iframe width="720" height="720" src="https://www.youtube.com/embed/oz7OPl9EMhg?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Prompt: Help Me Debug a Cluster! - Anusha Ragunathan &amp; Lili Wan, Intuit Inc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 35.Prompt: Help Me Debug a Cluster! - Anusha Ragunathan &amp; Lili Wan, Intuit Inc

Anusha Ragunathan and Lili Wan from Intuit Inc delve into advanced strategies for debugging Kubernetes clusters, focusing on reducing the Mean Time to Detection (MTD) and Mean Time to Resolution (MTTR) of issues. Their approach combines traditional monitoring with AI-driven insights to streamline the troubleshooting process.

**Overview and Challenges:**
- **Intuit's Scale:** The presentation begins with an overview of Intuit's extensive use of Kubernetes, highlighting the challenge of managing 315 clusters with 30,000 namespaces, catering to 7,000 developers.
- **Observation Hurdles:** Regular cluster activities, combined with significant change events like upgrades and peak season preparations, make observation and troubleshooting complex.

**Cluster Golden Signals:**
- Intuit adopted a methodology to distill the flood of alerts into actionable insights through what they term "Cluster Golden Signals," akin to the "Golden Signals" in service monitoring.
- These signals focus on four key pillars: error rates, traffic, latency, and saturation, offering a unified health metric for clusters.
- By categorizing cluster components and defining critical health signals, they were able to create dashboards and alerts that significantly reduce noise and focus on genuine issues.

**AI for Platform Debugging:**
- **Integration with AI:** To tackle the challenge of identifying root causes and remediation steps, Intuit leveraged AI, specifically large language models (LLMs), to process error data and suggest fixes.
- **KGPT:** An open-source tool, KGPT scans Kubernetes clusters for issues, pulling diagnostic information like pod logs and events, then queries LLMs for remediation advice.
- **Private Embeddings:** Recognizing the limitations of public AI models in understanding Intuit-specific configurations and issues, they also utilized an internal AI service to pull insights from private documentation and runbooks.

**Implementation and Workflow:**
- A Prometheus setup triggers alerts based on predefined rules. When specific thresholds are met, indicating a potential issue, a secondary check within the cluster is initiated.
- This secondary check uses KGPT to perform a deeper analysis, querying both public LLMs and Intuit's private AI service for remediation steps.
- The findings are aggregated and stored, ready for review by platform engineers, significantly aiding in the rapid identification and resolution of issues.

**Demo and Key Insights:**
- The demo illustrated how this integrated system works in real-time, from the triggering of an alert through to the AI-driven analysis and the presentation of potential fixes.
- **Reduced Alert Fatigue:** By focusing on Cluster Golden Signals, Intuit was able to cut down on the overwhelming number of alerts and highlight those with genuine impact.
- **Faster Debugging:** The use of AI for suggesting remediation steps has shown promising results, offering faster pathways to identify and fix issues.
- **Community Engagement:** Intuit is actively contributing back to the KGPT community, suggesting improvements and new features, demonstrating the collaborative potential of this approach.

<iframe width="720" height="720" src="https://www.youtube.com/embed/pCgLYXevN3Y?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="How KubeVirt Improves Performance with CI-Driven Benchmarking, and You Can Too" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 36.How KubeVirt Improves Performance with CI-Driven Benchmarking, and You Can Too

The presentation by Ryan Hallisy and Al Patel from Nvidia focuses on the implementation and benefits of Continuous Integration (CI)-driven benchmarking within the KubeVirt project. KubeVirt is a Kubernetes add-on that allows users to run and manage virtual machine workloads alongside container workloads. The talk outlines the strategies and tools employed to measure and enhance the scalability and performance of KubeVirt, providing insights that can be applied to other Kubernetes operators or extensions.

**KubeVirt and Performance Benchmarking:**
- **KubeVirt Basics:** KubeVirt extends Kubernetes by adding virtual machine management capabilities, allowing VMs to run inside containers.
- **Benchmarking Importance:** Regular benchmarking through the CI pipeline is crucial for detecting performance regressions and improvements over time.

**Benchmarking Stack Components:**
1. **Workload Generation:** Utilizing tools like KubeBurner to simulate real-world workloads within the Kubernetes cluster.
2. **Metrics Monitoring:** Leveraging Prometheus for continuous metrics monitoring, focusing on critical aspects such as API server requests and container resource usage.
3. **CI Automation:** Integrating benchmarking tests into the CI pipeline to automatically trigger performance and scalability assessments with every significant code change.

**Scalability Dimensions and Thresholds:**
- **Environment Factors:** Benchmarking must consider the environment, including Kubernetes versions, control plane configurations, and resource allocations.
- **Scalability Thresholds:** Identifying critical scalability metrics for KubeVirt, such as the number of virtual machines, pods, and persistent volume claims, to define acceptable performance levels.
- **Extensions Impact:** Understanding how Kubernetes extensions and additional controllers (like KubeVirt) impact overall cluster scalability and API server load.

**Benchmarking Results Utilization:**
- **Performance Trends:** Graphical representation of performance metrics over time, highlighting the impact of Kubernetes and KubeVirt updates on VM creation times and API server load.
- **Scalability Insights:** Analysis of how changes in the cluster environment or KubeVirt configurations affect the cluster's ability to handle large numbers of resources and concurrent operations.

**Adopting CI-Driven Benchmarking:**
- The presentation illustrates how other projects can adopt KubeVirt's benchmarking methodologies to monitor and improve their own performance and scalability.
- **Tooling Recommendations:** Suggestions for open-source tools and practices that can be used to set up a similar benchmarking framework in other projects.

**Conclusion and Takeaways:**
- CI-driven benchmarking is a powerful approach to maintaining high performance and scalability in Kubernetes-based applications.
- KubeVirt's integration of benchmarking into its CI pipeline serves as a model for other projects looking to ensure their extensions do not negatively impact the broader Kubernetes ecosystem.

<iframe width="720" height="720" src="https://www.youtube.com/embed/rphD1vCarNQ?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Kubernetes as a Data Platform - Robert Hodges, Edith Puclla, Clayton Coleman" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 37.Kubernetes as a Data Platform - Robert Hodges, Edith Puclla, Clayton Coleman

The discussion, led by experts from diverse backgrounds in the Kubernetes ecosystem, delves into the evolution and future of Kubernetes as a comprehensive data platform, particularly focusing on databases, batch processing, and AI/ML workloads.

**Kubernetes and Databases:**
- **Progress and Acceptance:** Initially skeptical, the tech community has significantly warmed up to the idea of running databases on Kubernetes, evidenced by less questioning and more implementation.
- **Key Enablers:** Features like StatefulSets and PersistentVolumes have matured, facilitating database operations on Kubernetes. Operator patterns have also emerged as crucial for managing database lifecycles and ensuring high availability.

**Batch Processing in Kubernetes:**
- **Historical Context:** Initially, Kubernetes focused on stateless applications, leaving batch processing as a later consideration.
- **Recent Improvements:** Tools like Kube-burner and the introduction of JobSets reflect Kubernetes' growing capability to handle complex batch processing tasks efficiently.

**AI and Machine Learning Workloads:**
- **Integration and Support:** Kubernetes has seen an uptick in AI/ML workload management, supported by tools like Kubeflow and improved handling of large data sets and specialized hardware like GPUs.
- **Community and Tools:** The open-source nature and the robust community around Kubernetes have played a pivotal role in its adaptation to AI and ML needs, with continuous improvements expected.

**Forward-Looking Statements:**
- **Future of Kubernetes:** Panelists envision Kubernetes evolving to better support data-intensive applications, highlighting the need for improvements in storage-compute separation, batch processing, and AI/ML workload management.
- **The Next Decade:** Expectations include Kubernetes becoming more streamlined and efficient, with a focus on flattening the complexity and enhancing integrations with Linux and hardware advancements.

**Concluding Thoughts:**
- **Kubernetes as a Data Platform:** The discussion underscores Kubernetes' significant evolution from its initial focus on stateless applications to a versatile platform capable of handling stateful, batch, and AI/ML workloads.

<iframe width="720" height="720" src="https://www.youtube.com/embed/tZhDQpmzaS0?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Object Storage on Kubernetes? Completed It with Provider Ceph. - Conor Nolan &amp; Richard Kovacs" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 38.Object Storage on Kubernetes? Completed It with Provider Ceph. - Conor Nolan &amp; Richard Kovacs

Conor Nolan and Richard Kovacs from Akamai Technologies present an insightful exploration into integrating object storage solutions with Kubernetes through Provider Ceph, a crossplane provider developed to manage S3 buckets in Ceph clusters from within Kubernetes. Their journey from the conceptual phase through to development and deployment unveils the challenges faced, innovative solutions implemented, and lessons learned, shedding light on the intricate relationship between Kubernetes, Ceph, and the broader cloud-native landscape.

**Background and Motivation:**
- Nolan and Kovacs’ experience with Kubernetes-native block storage at a previous startup informed their approach to tackling a new challenge at Akamai: managing S3 buckets across multiple distributed Ceph clusters via a single Kubernetes cluster.

**Solution and Architecture:**
- **Crossplane Adoption:** The decision to utilize Crossplane for this project stemmed from its ability to extend Kubernetes with capabilities to manage external resources, fitting perfectly with their needs.
- **Provider Ceph Development:** They developed Provider Ceph to make Kubernetes aware of Ceph clusters and their resources, leveraging Crossplane as the framework for building this provider.
- **Event-Driven Design:** The architecture emphasizes an event-driven, eventually consistent model for managing S3 bucket states across distributed systems, fitting seamlessly with Kubernetes' paradigm.

**Key Features and Implementations:**
- **Custom Resource Representations:** Each S3 bucket is represented as a custom resource within Kubernetes, serving as the source of truth and facilitating the reconciliation process to align desired state with actual state across Ceph clusters.
- **Asynchronous Operations:** Handling S3 operations asynchronously was crucial due to the interaction with multiple backends, necessitating visibility into the operations' success or failure.
- **Scalability Considerations:** The system was designed to potentially handle upwards of 100,000 buckets, necessitating efficient scalability and performance optimization strategies.

**Challenges and Insights:**
- **Complexity in Reconciliation:** Developing a Kubernetes operator to manage external resources introduced complexity, especially when encapsulating all CRUD operations within a single reconcile loop. Crossplane's abstraction layer significantly reduced this complexity.
- **Visibility and Monitoring:** Custom conditions and health checks were implemented to provide granular visibility into the health and state of both individual buckets across clusters and the overall system.

**Demonstration and Performance:**
- A live demo highlighted the dynamic management of S3 buckets, including handling cluster failures and bucket synchronization across multiple Ceph backends, showcasing the practical effectiveness of their solution.
- The scalability and performance discussion emphasized the importance of considering etcd performance, Kubernetes API server limitations, and the necessity of design choices driven by these constraints for large-scale deployments.

**Future Directions:**
- Nolan and Kovacs outlined future enhancements, including seamless integration with distributed control planes like KCP or Karada, aiming for a single entry point and the ability to manage an "infinite" number of buckets, all atop cloud-native technologies.

<iframe width="720" height="720" src="https://www.youtube.com/embed/vKfCl3x_Xes?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Increasing GPU Utilisation on K8s Clusters Dedicated for AI/ML Workloads" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 39.Increasing GPU Utilisation on K8s Clusters Dedicated for AI/ML Workloads

This presentation by Maciek Mazur, a Principal AI Engineer at Canonical, explores strategies and insights for maximizing GPU utilization within Kubernetes clusters, specifically for AI/ML workloads. The talk sheds light on the MLOps lifecycle, GPU essentials, and innovative projects enhancing Kubernetes' capability to manage complex AI/ML tasks effectively.

**MLOps Lifecycle and Kubernetes:**
- **Structured Approach:** AI/ML engineers follow a structured MLOps process involving various computational steps, for which Kubernetes provides scalability, repeatability, and portability.
- **GPU Necessity:** GPUs are vital for AI/ML due to their efficiency in matrix multiplication, a common operation in machine learning models.

**GPU Utilization Challenges:**
- **Selection and Performance:** Choosing the right GPU is crucial, as there's a significant variation in performance per cost across different models.
- **Resource Sharing:** Sharing expensive GPU resources among multiple users and teams is essential for cost efficiency.

**Technological Solutions and Projects:**
- **Paddle Paddle and Volcano:** For splitting larger AI/ML tasks into manageable units and efficient scheduling, projects like Paddle Paddle and Volcano are utilized.
- **Armada for Multi-Cluster Management:** Armada extends Kubernetes' capability to handle AI/ML workloads across multiple clusters by optimizing resource allocation and job scheduling.

**Real-World Implementation Insights:**
- **Public Sector Use Cases:** The discussion includes experiences from large-scale public sector projects, where Kubernetes clusters manage between 10,000 to 15,000 GPUs, highlighting the need for open-source solutions and strict security and SLA requirements.
- **Hardware Mix for Optimal Performance:** A mix of high-performance, mid-range, and low-end GPUs, along with network optimization technologies like InfiniBand and Spectrum X, is recommended for balancing performance needs and cost efficiency.
- **Scheduling and Resource Allocation:** Advanced scheduling techniques, including GPU slicing and gang scheduling, are crucial for maximizing GPU utilization across diverse workloads and teams.

**Key Recommendations:**
- **Diverse Hardware and Networking:** A varied hardware setup coupled with dedicated networking for GPU traffic ensures efficient resource use and job execution.
- **Metal as a Service (MaaS) and Flexibility:** Utilizing MaaS for bare-metal management and flexible resource pooling allows dynamic allocation and reallocation of resources based on demand and utilization patterns.
- **Observability and Reconfiguration:** Continuous monitoring of system utilization and performance enables regular system reconfigurations, ensuring optimal use of available resources.

**Conclusion:**
The session emphasizes the importance of a strategic approach to GPU utilization within Kubernetes for AI/ML workloads, advocating for the use of specific tools and practices to address the unique challenges posed by high-performance computing demands. The integration of projects like Paddle Paddle, Volcano, and Armada, along with a thoughtful hardware selection, plays a pivotal role in enhancing the efficiency and effectiveness of AI/ML operations on Kubernetes.

<iframe width="720" height="720" src="https://www.youtube.com/embed/DMwPjsG4wIM?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="The Leading Edge of AuthN and AuthZ by Keycloak - Takashi Norimatsu &amp; Thomas Darimont" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 40.The Leading Edge of AuthN and AuthZ by Keycloak - Takashi Norimatsu &amp; Thomas Darimont

Takashi Norimatsu and Thomas Darimont explore the advancements in authentication (AuthN) and authorization (AuthZ) facilitated by Keycloak, highlighting its latest security features and potential integrations. Their presentation delves into new standards and practices that are shaping the future of secure, user-centric online interactions.

**Keycloak's Evolution and Security Enhancements:**
- **Introduction:** Keycloak, an open-source identity and access management solution, has become a CNCF incubating project, continuously adding features and support for various open standards for AuthN and AuthZ.
- **Passkeys:** Keycloak's recent updates embrace Passkey authentication, moving away from traditional passwords towards a more secure and user-friendly method using cryptographic keys. This includes support for both synchronous and device-bound Passkeys, facilitating cross-device and same-device authentication.
- **Oauth 2.1 Support:** Keycloak's adoption of OAuth 2.1 represents a significant enhancement in securing authorization processes. OAuth 2.1, still in draft status, aims to harden the security aspects of OAuth 2.0 by introducing measures against the misuse of refresh and access tokens.
- **Distributed Identity through Decentralized IDs (DIDs):** The discussion also touched on the emerging paradigm of self-sovereign identity (SSI) and decentralized IDs (DIDs). Keycloak's efforts to support OpenID for Verifiable Credentials (OIDC4VC) align with the European Digital Identity Wallet framework, indicating a shift towards more controlled and user-owned identity information.

**Practical Demonstration and Use Cases:**
- **Integration with Open Policy Agent (OPA):** Thomas Darimont presented a practical integration of Keycloak with OPA, showcasing how Keycloak can act as a Policy Enforcement Point (PEP) and leverage OPA as a Policy Decision Point (PDP) for flexible authorization checks. This integration allows for policies as code, enabling version control, testing, and auditing of authorization rules.
- **Declarative Policy Language (Rego):** Utilizing Rego, OPA's policy language, the demonstration illustrated how to define access policies for Keycloak-managed applications. This method supports fine-grained access control, such as client-specific rules and conditions based on user roles, group memberships, and IP addresses.
- **Client Policies and Profiles:** The talk highlighted the use of Keycloak's client policies and profiles to apply access checks dynamically across different applications and authentication methods. This feature enhances security by enforcing consistent policy evaluations and decisions, tailored to specific client interactions within Keycloak.

**Conclusions and Insights:**
- **Enhanced Security and Flexibility:** Keycloak's ongoing development and integration with tools like OPA reflect a strong commitment to enhancing security, flexibility, and user control in the realm of AuthN and AuthZ.
- **Future-Ready Identity Management:** By embracing emerging standards like OAuth 2.1 and DIDs, Keycloak is positioning itself at the forefront of identity management, catering to the evolving needs of both developers and end-users.
- **Community Involvement:** The presentation underscored the importance of community contributions to Keycloak's growth and encouraged participation in its development, particularly in areas like OIDC4VC support and OAuth 2.1 compliance.

<iframe width="720" height="720" src="https://www.youtube.com/embed/NcA9VnFzD-0?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Enhancing Reliability Through Multi-Cluster Deployment: Leveraging the Power of Karmada" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 41.Enhancing Reliability Through Multi-Cluster Deployment: Leveraging the Power of Karmada

This session, led by experts from Huawei and DaoCloud, focused on Karmada, a CNCF project aimed at managing Kubernetes resources across multiple clusters to enhance reliability and efficiency in multi-cloud and hybrid cloud environments.

**Key Topics Discussed:**

- **Multi-Cluster Adoption Trends:** The session began with an overview of the increasing adoption of multi-cloud strategies by organizations, emphasizing the need for effective management of Kubernetes resources across various environments to ensure seamless operation and reduce management complexities.

- **Introduction to Karmada:** Karmada (Kubernetes Armada) is introduced as an advanced solution designed to simplify the deployment and management of applications across different Kubernetes clusters, regardless of their location in public or private clouds.

- **Core Features of Karmada:**
  - **Cluster Management:** Karmada provides mechanisms for easily adding, removing, and managing Kubernetes clusters, including support for different registration modes to accommodate various network access scenarios.
  - **Cross-Cluster Scheduling:** Through policies, Karmada allows users to define how applications should be distributed across clusters, taking into account the unique requirements and configurations of each cluster.
  - **Resource Propagation and Override Policies:** These features enable applications to be deployed across clusters with customized configurations, addressing the diverse requirements of multi-cloud deployments.
  - **Disaster Recovery and High Availability:** Karmada enhances application reliability by enabling failover and migration capabilities across clusters, ensuring continuous service availability.
  - **Unified Authentication and Authorization:** By providing a centralized authentication mechanism, Karmada simplifies access control and management across multiple clusters.

- **Challenges in Multi-Cloud Management:** The speakers highlighted the challenges faced by organizations in managing multiple Kubernetes clusters, such as increased operational complexity and the need for unified management and orchestration solutions.

- **Karmada's Community and Development:** The session also touched on the vibrant community behind Karmada, showcasing its growth and the contributions from over 500 developers. The project's roadmap and future development plans were discussed, highlighting the focus on enhancing features like AI training support and building multi-cloud federated learning workflows.

- **Practical Use Cases and Demonstrations:** Throughout the presentation, practical use cases and demonstrations were provided to showcase how Karmada can be utilized to manage applications across clusters efficiently, including scenarios involving cross-cluster service discovery and multi-cluster application failover.

<iframe width="720" height="720" src="https://www.youtube.com/embed/OpipEzAHxME?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="KubeEdge: Extending Kubernetes to the Edge with Real-World Industry Use Case" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 42.KubeEdge: Extending Kubernetes to the Edge with Real-World Industry Use Case

This presentation delivered by Fisher Xu from Huawei delves into the nuances of KubeEdge, a pivotal CNCF project designed to bridge Kubernetes with edge computing. Through this integration, KubeEdge aims to streamline the deployment and management of IoT and edge devices, enhancing the scalability and efficiency of edge computing tasks within the Kubernetes ecosystem.

**Key Highlights and Innovations:**

- **Project Evolution:** KubeEdge, since its inception in 2018, has evolved rapidly, being recognized as a CNCF Sandbox project, and striving towards CNCF graduation. Its development journey has been marked by continuous innovation, including the release of significant features and the establishment of sub-projects like EdgeMesh for networking and Sedna for AI enhancement at the edge.

- **KubeEdge Architecture:** The architecture of KubeEdge is designed to extend Kubernetes from the cloud to the edge, optimizing edge computing capabilities. It integrates three main components: CloudCore (control plane in the cloud), EdgeCore (node agent on edge devices), and DeviceTwin (for IoT device management), leveraging Kubernetes APIs for seamless operation across cloud and edge.

- **Distinctive Features:**
  - **Edge Autonomy:** KubeEdge enables edge nodes to operate independently, ensuring functionality even in scenarios of intermittent cloud connectivity, crucial for mission-critical applications.
  - **Efficient Resource Management:** It introduces mechanisms for resource optimization, including lightweight Kubernetes components for edge nodes, ensuring minimal resource consumption and fast response times.
  - **Device Management:** KubeEdge offers advanced IoT device management capabilities, facilitating the deployment, updating, and monitoring of edge devices from a central control plane.

- **Real-World Use Cases:**
  - **Satellite Data Processing:** One standout use case involves satellite data processing where KubeEdge orchestrates edge computing tasks in space, significantly reducing data transmission costs and latency.
  - **Smart Traffic Systems:** KubeEdge has been implemented in intelligent transportation systems, processing data from roadside units and cameras to improve traffic management and safety.
  - **Offshore Oil Fields:** In the energy sector, KubeEdge manages IoT devices across offshore oil fields, enhancing operational efficiency and safety through real-time data analysis and automation.

- **Community and Collaboration:** KubeEdge thrives on an active community, with contributions from numerous organizations worldwide. The project's open governance model fosters innovation and ensures the project's long-term sustainability and alignment with industry needs.

- **Future Directions:** Looking ahead, KubeEdge aims to further integrate AI and machine learning capabilities, expand its IoT device support, and enhance its networking solutions, underlining its commitment to advancing edge computing within the Kubernetes ecosystem.

<iframe width="720" height="720" src="https://www.youtube.com/embed/YTE7OvxotDE?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Unikernels in K8s: Performance and Isolation for Serverless Computing with Knative" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 43.Unikernels in K8s: Performance and Isolation for Serverless Computing with Knative

The presentation by Anastasios Nanos and his team at a young research company delves into integrating unikernels with Kubernetes, specifically for serverless computing environments using Knative. Their exploration into this niche focuses on addressing the challenges of execution latency, throughput, energy efficiency, and particularly, the security and isolation of serverless functions. 

**Key Insights from the Talk:**

- **Serverless Platforms and System Software:** The necessity for serverless platforms where developers can trigger functions based on events without managing the underlying infrastructure. The team's research highlighted concerns over the system software stack's impact on execution latency and security in serverless frameworks.

- **Unikernels for Enhanced Performance and Isolation:** Unikernels, defined as specialized, single-address-space machine images, promise lower latency and improved security through their architecture. Despite skepticism around their readiness for production, recent developments have broadened their library support and tools, pushing towards their adoption.

- **Challenges with Traditional Containers:** While containers offer numerous benefits in cloud-native ecosystems, they pose security and isolation issues when managing untrusted code. Sandbox container runtimes like Kata Containers and gVisor attempt to address this but introduce overheads, affecting cold boot times.

- **Developing a Unikernel-Compatible Container Runtime (uranC):** The team's solution, uranC, treats unikernels as OCI artifacts, allowing Kubernetes to manage unikernels as it does containers. This approach provides the necessary isolation while minimizing the startup latency associated with sandboxed containers.

- **Benchmarking Unikernel Performance in Serverless Functions:** Their findings indicated that unikernels could match the performance of standard containers in terms of service response latency, without the security concerns. Additionally, unikernels showed significant improvements over sandboxed container runtimes in both latency and resource efficiency.

- **Practical Demonstrations and Future Work:** The presentation included demonstrations of building unikernel images and deploying them in Kubernetes clusters, showcasing the practical application of their research. The team also outlined plans for future enhancements, including further reducing startup times and expanding cloud-native unikernel tools.

**Conclusions and Takeaways:**

- **Potential of Unikernels in Kubernetes:** Unikernels present a viable solution for running serverless workloads in Kubernetes with enhanced security and efficiency. Their integration into the Kubernetes ecosystem through uranC opens up new possibilities for deploying high-performance, isolated functions.

- **Open Source Contributions and Community Engagement:** The team emphasized the importance of community contributions to their project, encouraging participation in developing and refining unikernel tools for Kubernetes.

<iframe width="720" height="720" src="https://www.youtube.com/embed/YpQxxZ1Izek?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="SIG API Machinery Maintainers (Two Tracks) - Abu Kashem, Red Hat &amp; Mike Spreitzer, IBM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 44.SIG API Machinery Maintainers (Two Tracks) - Abu Kashem, Red Hat &amp; Mike Spreitzer, IBM

This presentation delves into the critical advancements and operational insights into Kubernetes API server's Priority and Fairness (PF) feature, with Abu Kashem and Mike Spreitzer leading the discussion. Their comprehensive review underscores the evolution of Kubernetes' API management, focusing on ensuring equitable access and preventing API server overload.

**Key Features and Evolution:**
- **Priority and Fairness (PF):** A mechanism within the Kubernetes API server that manages request load, ensuring no single controller can overwhelm the API server by regulating client request rates.
- **Historical Context:** Transition from client-side throttling to PF, marking a significant shift in how Kubernetes manages API server load and request fairness.

**PF's Mechanisms and Benefits:**
- **Request Classification:** Based on authentication and access control attributes, ensuring accuracy and difficulty in circumvention.
- **Concurrency Regulation:** Considers request rate and execution time, focusing on maintaining a balance between active request numbers and server capacity.
- **Granularity and Configuration:** Offers detailed control over request handling, replacing the simpler Max-in-flight filter with a system that allows for nuanced request categorization and queue management.

**Operational Insights:**
- **Queuing and Dispatching:** Introduces a fair queuing system for managing request queues and dispatches based on configured priorities, ensuring balanced load distribution across different request types.
- **Dynamic Adaptation:** The system periodically adjusts concurrency limits through a borrowing mechanism, allowing for flexible response to varying load conditions.
- **Comprehensive Configuration:** The PF feature is highly customizable through Kubernetes API objects, enabling tailored management of request prioritization and handling.

**Real-World Implementation and Use Cases:**
- **API Server Protection:** Demonstrates PF's role in safeguarding the API server from overloading by effectively managing request loads, ensuring system reliability and performance.
- **Fair Access Across Controllers:** Highlights how PF maintains equitable access to the API server for all controllers, preventing any single entity from monopolizing server resources.
- **Enhanced Monitoring and Visibility:** Discusses the introduction of metrics for monitoring PF's performance, aiding in the identification and resolution of potential issues.

**Community Involvement and Future Directions:**
- The presentation emphasizes the importance of community contributions to the ongoing development and refinement of the PF feature. It also touches on future enhancements aimed at improving fairness, efficiency, and configurability.

**Challenges and Considerations:**
- **Complexity in Configuration:** Acknowledges the intricate nature of setting up and maintaining PF configurations, underscoring the need for careful planning and understanding of Kubernetes' API management principles.
- **Adapting to Evolving Needs:** The session reflects on the necessity to continually adapt and refine PF mechanisms to meet the changing demands of Kubernetes environments and workloads.

<iframe width="720" height="720" src="https://www.youtube.com/embed/Yp_kiukE5gE?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Saving the Planet One Cluster at a Time: Operationalising Sustainability in Kubernetes" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 45.Saving the Planet One Cluster at a Time: Operationalising Sustainability in Kubernetes

This insightful presentation by Gabby and Brendan from resync, a consultancy focused on reducing IT's environmental impact, sheds light on operationalizing sustainability within Kubernetes environments. Their discussion traverses the challenges and solutions associated with minimizing the carbon footprint of IT infrastructure, specifically targeting Kubernetes clusters.

**Why It Matters:**
- **Global Impact:** The IT sector's energy consumption ranges from 5 to 10% of the global total, translating to 2 to 5% of worldwide greenhouse gas emissions. This footprint underscores the pressing need for sustainable IT practices.
- **Infinite Cloud Illusion:** The cloud's scalability has led to redundancy and waste, particularly in server utilization, amplifying the industry's environmental impact.

**Carbon Equivalents and Sustainability Metrics:**
- **Understanding CO2e:** Carbon equivalents (CO2e) measure the impact of various greenhouse gases relative to carbon dioxide, providing a unified metric for assessing environmental impact.
- **Operational vs. Embodied Emissions:** Total carbon emissions include operational emissions from energy use and embodied emissions from manufacturing and end-of-life processes. Strategies for reduction focus on optimizing both aspects.

**Operational Emissions Insights:**
- **Calculating Impact:** Operational emissions are calculated by combining energy consumption data with grid carbon intensity, a measure of how much carbon is emitted per unit of electricity generated.
- **Tools and Techniques:** Tools like smart plugs and Linux's Running Average Power Limit (RAPL) provide ways to measure energy consumption directly, while software tools can estimate virtualized systems' emissions based on known hardware profiles.

**Strategies for Reduction:**
- **Time Shifting Workloads:** Adjusting workloads to run during periods of lower carbon intensity on the grid can significantly reduce emissions.
- **Geographical Load Shifting:** Deploying workloads in regions with greener energy sources offers another avenue for reduction.
- **Server Lifespan:** Extending the lifespan of servers reduces the relative impact of embodied emissions.

**Implementation and Experimentation:**
- Gabby and Brendan showcased experiments measuring Kubernetes cluster energy consumption under different loads and configurations, demonstrating the effectiveness of their strategies.
- **Node Utilization:** Higher node utilization was correlated with more efficient energy use, emphasizing the importance of optimizing resource requests in Kubernetes manifest files.
- **Autoscaling Impact:** Properly configured autoscalers can reduce energy consumption by aligning resource allocation more closely with actual needs.

<iframe width="720" height="720" src="https://www.youtube.com/embed/YqdskZNbsgs?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="SIG-Scheduling Intro &amp; Deep Dive - Wei Huang, Apple &amp; Kante Yin, DaoCloud" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 46.SIG-Scheduling Intro &amp; Deep Dive - Wei Huang, Apple &amp; Kante Yin, DaoCloud

The session led by Wei Huang and Kante Yin delves into the intricate mechanisms of Kubernetes' scheduling system, focusing on recent updates and sub-project initiatives aimed at enhancing the flexibility, efficiency, and overall capability of the scheduling process.

**Key Insights and Developments:**

- **Scheduling Framework Evolution:** Since Kubernetes 1.19, the introduction of the Scheduling Framework has allowed for more sophisticated and customizable scheduling decisions. This framework serves as the backbone for all current default and custom scheduling plugins.

- **Notable Features and Updates:**
  - **Preemption and Queue Management:** Detailed explanations were provided on how Kubernetes handles pod scheduling when resources are scarce, including preemption logic and the separation of scheduling and binding cycles for efficiency.
  - **Recent Cap (Kubernetes Enhancement Proposal) Updates:**
    - **Part Scheduling Readiness:** Introduced in Kubernetes 1.26 (Alpha) and reaching GA in 1.30, this feature allows for delaying a pod's entry into the scheduling queue until certain conditions are met, enhancing control over when a pod should be scheduled.
    - **Queue Hinting for Efficient Scheduling:** Enhancements in how Kubernetes schedules pods more efficiently by reducing unnecessary queue operations, thereby optimizing memory usage and improving overall system performance.
    - **Enhanced Node Affinity and Anti-Affinity:** Updates to the scheduling algorithms allow for more granular control over pod placement relative to other pods and nodes, addressing the dynamic nature of cloud environments.

- **Sub-Project Highlights:**
  - **C Scheduler (Cluster Simulator):** A tool for simulating Kubernetes clusters to test different scheduling scenarios without the need for physical resources. Recent updates include simulation support for CPU and memory usage, making it a powerful tool for development and testing.
  - **Queue Job Scheduling:** Queue continues to evolve with features like hierarchical queues for complex job scheduling scenarios, addressing the needs of diverse compute workloads in multi-tenant environments.
  - **Wasm (WebAssembly) Scheduling Extensions:** Introduction of WebAssembly-based dynamic scheduling plugins, offering a sandboxed environment for custom scheduler logic without the need to recompile the main scheduler.

**Challenges and Future Directions:**
- **Scalability and Efficiency:** Ongoing efforts to improve the scalability of the Kubernetes scheduler, addressing the challenges of managing thousands of nodes and pods in large-scale deployments.
- **Community and Collaboration:** The session underscored the importance of community contributions to the evolution of Kubernetes scheduling, inviting engagement and innovation from developers around the world.

<iframe width="720" height="720" src="https://www.youtube.com/embed/ZD2FzR_xFdk?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="SIG-Multicluster Intro and Deep Dive - Jeremy Olmsted-Thompson, Google &amp; Stephen Kitt, Red Hat" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 47.SIG-Multicluster Intro and Deep Dive - Jeremy Olmsted-Thompson, Google &amp; Stephen Kitt, Red Hat

Jeremy Olmsted-Thompson and Stephen Kitt offer a comprehensive overview of SIG-Multicluster's work, focusing on Kubernetes' ability to manage resources across multiple clusters. Their talk highlights recent initiatives and provides insights into future directions aimed at enhancing Kubernetes' multicluster capabilities.

**SIG-Multicluster's Focus:**
- **Broad Scope:** Addressing the growing necessity for Kubernetes deployments to span across multiple clusters, for reasons such as fault tolerance, policy requirements, and data locality.
- **Challenges of Multi-Cluster:** Emphasizes Kubernetes' initial design for single-cluster use and the ongoing efforts to extend functionalities for multicluster operations.

**Key Initiatives and APIs:**
- **Cluster ID and Cluster Set:** Introduction of mechanisms to identify clusters and cluster sets, allowing for standardized cluster management and operation within a unified set.
- **Multi-Cluster Services (MCS):** Progress on APIs that facilitate service discovery and connectivity across clusters, enhancing the user experience in deploying and managing distributed services.
- **Scheduling and Resource Management:** Insights into the complexities of ensuring optimal resource utilization and efficient scheduling across multiple clusters.

**Technical Deep Dives:**
- **Cluster Sets Concept:** Explores the principle of "cluster sets," emphasizing governance, trust, and namespace sameness to enable coherent multicluster strategies.
- **APIs for Cluster Identity:** Details on the APIs developed to identify clusters and their roles within a cluster set, supporting more sophisticated multicluster operations.

**Operational Challenges and Solutions:**
- **Visibility and Discovery:** Discusses the challenges of maintaining visibility and enabling discovery of services across clusters, outlining current solutions and potential future improvements.
- **Security and Isolation:** Highlights the ongoing work to address security concerns and ensure proper isolation of resources and operations across cluster boundaries.

**Future Directions:**
- **Enhanced APIs and Tools:** Indicates plans for the continued development of APIs and tools that will further simplify the management of Kubernetes resources across multiple clusters.
- **Community Engagement:** Calls for community involvement to share use cases, contribute to ongoing projects, and help shape the future of Kubernetes multicluster capabilities.

**Community and Collaboration:**
- **Open Governance and Contributions:** Encourages participation from the Kubernetes community in SIG-Multicluster activities, emphasizing the importance of feedback and contributions in driving the project forward.
- **Building on Real Needs:** Stresses the focus on developing solutions based on concrete needs and use cases, aiming for practical and widely applicable multicluster strategies.

<iframe width="720" height="720" src="https://www.youtube.com/embed/ZykwwHt5hYY?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="At the Intersection of Cilium CNI and Service Mesh - Who Has the Right of Way? - Christine Kim" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 48.At the Intersection of Cilium CNI and Service Mesh - Who Has the Right of Way? - Christine Kim

Christine Kim's presentation offers an in-depth analysis of the integration and functionalities of Cilium as a Container Network Interface (CNI) within Kubernetes clusters, particularly focusing on its interaction with service mesh technologies. The discussion navigates the complexities of network management, aiming to clarify the distinctions and synergies between CNIs and service meshes, using Cilium as a case study.

**CNI Fundamentals and Cilium's Role:**
- **CNI Overview:** Container Network Interfaces (CNIs) are crucial for enabling container-to-container communication within Kubernetes clusters, laying the groundwork for network connectivity.
- **Cilium Introduction:** Cilium is highlighted as a powerful CNI leveraging eBPF (Extended Berkeley Packet Filter) technology to provide advanced networking features, security, and observability.

**Cilium's Features and Capabilities:**
- **eBPF-based Networking:** Utilizes eBPF to efficiently manage network policies, load balancing, and routing at both the kernel and application layers.
- **Network Policy Enforcement:** Cilium enhances security through granular network policies that govern pod-to-pod communication across Kubernetes clusters.
- **Performance and Scalability:** Offers significant performance benefits, particularly for high-throughput network traffic, by optimizing data paths within the kernel.

**Service Mesh Context and Comparison:**
- **Service Mesh Defined:** Service meshes manage communication between services in a microservices architecture, offering features like service discovery, traffic management, and security.
- **Overlap and Distinction:** While CNIs like Cilium focus on lower-level network connectivity and security, service meshes operate at a higher level, managing service-to-service communication with an emphasis on application-level protocols and policies.

**Integration of Cilium with Service Meshes:**
- **Complementary Functions:** Cilium can operate alongside service meshes, providing a robust foundation for network traffic control and security that complements the application-level functionalities offered by service meshes.
- **Enhanced Security Posture:** The combination of Cilium's network-level security capabilities with the service-level security features of service meshes strengthens the overall security posture of Kubernetes deployments.

**Operational Insights and Recommendations:**
- **Deployment Considerations:** Kim provides practical advice on deploying Cilium within Kubernetes clusters, emphasizing considerations for compatibility and performance optimization with service meshes.
- **Use Case Applications:** Illustrates how Cilium can be applied in real-world scenarios, such as improving network performance, enhancing security with microsegmentation, and facilitating seamless multi-cloud connectivity.

**Community and Future Developments:**
- **Active Development:** Highlights the active development of Cilium and its growing community, inviting participation and collaboration to drive future enhancements.
- **Innovation at the Intersection:** The talk underscores ongoing innovations at the intersection of CNIs and service meshes, suggesting a bright future for integrated networking solutions in Kubernetes environments.

Christine Kim's session provides a comprehensive overview of Cilium's capabilities as a CNI and its interplay with service mesh technologies, offering valuable insights for Kubernetes users looking to enhance network performance, security, and observability. The discussion encourages further exploration and adoption of these technologies to address complex networking challenges in cloud-native architectures.

<iframe width="720" height="720" src="https://www.youtube.com/embed/_RthQ01bwU8?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Production-Ready AI Platform on Kubernetes - Yuan Tang, Red Hat" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 49.Production-Ready AI Platform on Kubernetes - Yuan Tang, Red Hat

Yuan Tang's comprehensive talk provides invaluable insights into building a production-ready AI platform on Kubernetes, exploring the AI landscape, ecosystem considerations, and practical implementations using various tech stacks. Tang, a principal engineer at Red Hat and project lead for Argo and Kubeflow, shares his expertise in deploying AI workloads effectively within Kubernetes environments.

**AI Landscape and Ecosystem:**
- **Evolving Field:** The AI field is rapidly growing, especially with the advent of large language models like GPT, blurring the lines between data science, machine learning, and deep learning.
- **Tools and Frameworks:** Tang discusses the importance of various projects such as Kubeflow, Argo, TensorFlow, PyTorch, and Volcano in the cloud-native AI ecosystem.

**Production Readiness Elements:**
- **Key Considerations:** Tang emphasizes the importance of scalability, reliability, observability, and flexibility when building AI platforms on Kubernetes, suggesting that there are many facets to consider in a production environment.
- **Reference Platform:** A reference platform is introduced, utilizing a tech stack that includes components for different AI workloads.

**Scalability:**
- **Horizontal and Vertical Scaling:** Discusses Kubernetes' horizontal and vertical scaling capabilities, including auto-scaling based on utilization metrics and event-driven approaches.
- **Algorithm Scalability:** Certain algorithms may not be suitable for large-scale operations without modifications for online processing.
- **Hardware Acceleration:** The talk highlights the importance of sharing resources between hardware accelerators like GPUs to enhance machine learning workflows.

**Reliability:**
- **High Availability and Disaster Recovery:** Leader election and standby replicas are crucial for ensuring the continuous operation of Kubernetes controllers.
- **Versioning and GitOps:** Emphasizes the role of version control and GitOps principles in maintaining a reliable and recoverable system.

**Observability:**
- **System and Statistical Metrics:** Tang points out the importance of both system-level and statistical metrics for monitoring the performance of machine learning models and infrastructure.
- **Model Explainability and Visualization:** The necessity of understanding and visualizing model behavior for trustworthiness and decision-making is discussed.

**Flexibility:**
- **Framework Support:** The platform should support multiple machine learning frameworks to meet diverse team requirements.
- **Standardized APIs:** Ensuring that the serving platform uses standardized protocols across different frameworks for stability and ease of onboarding.

**Practical Demonstration:**
- **Book Introduction:** Tang introduces his book, "Distributed Machine Learning Patterns," which includes patterns for building large-scale distributed machine learning systems and a reference implementation for hands-on experience.

**Conclusion:**
Tang's session provides a roadmap for deploying AI workloads on Kubernetes, highlighting the critical aspects of scalability, reliability, observability, and flexibility essential for a production-ready AI platform. The talk also showcases a reference implementation using Kubeflow, Argo, TensorFlow, PyTorch, and other tools, underscoring the power of Kubernetes in facilitating robust AI platforms.

<iframe width="720" height="720" src="https://www.youtube.com/embed/e8kvX6mRlyE?list=PLj6h78yzYM2N8nw1YcqqKveySH6_0VnI0" title="Thanos’ Infinity Stones and How You Can Operate Them! - Saswata Mukherjee &amp; Daniel Mohr, Red Hat" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

### 50. Thanos’ Infinity Stones and How You Can Operate Them! - Saswata Mukherjee &amp; Daniel Mohr, Red Hat

Saswata Mukherjee delivers a comprehensive exploration of Thanos, a distributed and scalable monitoring system, through a case study approach. This presentation at KubeCon highlighted innovative ways to utilize Thanos and the data within it to derive more value, comparing Thanos components to the Infinity Stones for their transformative power over monitoring practices.

**Thanos and Its Expansive Utility:**
- **Thanos Introduction:** As a distributed system built around Prometheus, Thanos provides global views, long-term storage, and querying capabilities for metrics across clusters, enhancing scalability and availability.
- **Architecture Overview:** Thanos consists of components like the Sidecar, attached to Prometheus instances for data reading and shipping; the Receiver, for handling metrics across network boundaries; and the Querier, for global data querying.

**Infinity Stones of Thanos:**
1. **Telemetry:** Thanos enables collecting data not only from your infrastructure but also from users and hardware products. This expands monitoring beyond traditional use cases, leveraging telemetry from diverse sources for more comprehensive insights.
2. **Analytics:** With the structured numerical data provided by Prometheus metrics, Thanos allows for various analytics applications, from customer data-backed decision-making to user experience enhancements and billing.
3. **Single Cluster Monitoring:** Differentiates platform and user workload metrics within a single Kubernetes cluster, improving clarity for operational teams and developers. Thanos facilitates this distinction, enhancing both observability and security.
4. **Multi-Cluster Use Cases:** Thanos excels in environments with multiple Kubernetes clusters, enabling dynamic management and providing a global view of metrics. The use of Thanos Receiver and Querier across clusters facilitates efficient, centralized monitoring.
5. **Operators for Automation:** Addressing the need for automation in managing Thanos components, initiatives like the Thanos operator and observatorium operator are highlighted for their potential to simplify deployments and optimize operations.
6. **Community Engagement:** The vibrant Thanos community plays a pivotal role in the project's development. Contributions, collaboration, and engagement within the community are crucial for driving innovations and addressing the evolving needs of users.

**Practical Applications and Demonstrations:**
- **Innovative Use Cases:** From satellite data processing and smart traffic systems to offshore oil fields, Thanos' flexibility and scalability enable a wide range of applications, demonstrating its capacity to handle demanding, data-intensive scenarios.

**Conclusion and Future Directions:**
- Thanos, with its modular architecture and community-driven development, continues to evolve, offering solutions that extend beyond traditional monitoring to include analytics, telemetry, and multi-cluster management. This session not only illuminated the technical aspects of Thanos but also showcased the strategic thinking necessary to leverage such a powerful tool effectively.



